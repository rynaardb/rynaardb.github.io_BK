[ { "title": "Leveraging AI for Enhanced CI/CD Pipeline Efficiency", "url": "/posts/leveraging-ai-for-enhanced-ci-cd-pipeline-efficiency/", "categories": "devops, ci-cd", "tags": "ai, artificial, intelligence, npl, natural-language-processing, ml, machine-learning, deep-learning, nural-network, devops, ci-cd, ci, cd, pipelines, build, automation", "date": "2023-06-19 00:00:00 +0200", "snippet": "In the constantly evolving landscape of software development, efficiency and automation are the driving forces. One of the areas that has seen significant strides in this regard is the implementati...", "content": "In the constantly evolving landscape of software development, efficiency and automation are the driving forces. One of the areas that has seen significant strides in this regard is the implementation of Continuous Integration and Continuous Delivery (CI/CD) pipelines. The integration of Artificial Intelligence (AI) in CI/CD has added another layer of automation and efficiency. In this post, we’ll explore how AI can streamline CI/CD pipelines, irrespective of the specific tools used.CI/CD Pipelines: A Quick RecapContinuous Integration (CI) is a practice that encourages developers to integrate their changes into a central repository frequently. Each check-in is then verified by an automated build, allowing teams to detect problems early. Continuous Delivery (CD), on the other hand, extends CI by ensuring that the code checked into the main repository is always in a deployable state.How AI Can Enhance CI/CD PipelinesArtificial Intelligence (AI) and Machine Learning (ML) are transforming a variety of industries and applications, and the area of software development, particularly Continuous Integration/Continuous Delivery (CI/CD) pipelines, is no exception. AI and ML can make these pipelines more efficient, proactive, and intelligent. Let’s delve deeper into the specific ways AI can enhance CI/CD pipelines.Intelligent Error Prediction and DetectionTraditional CI/CD pipelines rely on developers or operations teams to monitor logs and identify errors. This is a time-consuming task and might not always catch problems before they impact the application’s performance. AI can be used to predict potential errors and bottlenecks before they occur.Using historical data from the pipeline, Machine Learning algorithms can be trained to predict where failures are likely to occur based on patterns seen in past failures. This proactive approach can prevent many issues from becoming significant problems, reducing downtime and improving the overall reliability of the software.In addition to predicting potential errors, AI can automate the process of error detection. By learning from previous builds and deployments, AI algorithms can scan logs for anomalies and automatically classify them into types of errors. This allows teams to address and fix issues faster and more accurately.Test OptimizationTesting is a crucial phase in CI/CD pipelines, but not all tests are equal. Some are more likely to fail than others based on the changes made to the code. AI can help prioritize these tests, ensuring that the most critical tests are run first. This approach, known as risk-based testing, makes the testing process more efficient and allows teams to discover and fix bugs faster.Additionally, AI can be used to automate the generation of test cases based on the changes made to the code. By learning from past tests and code changes, AI can predict which parts of the code are likely to be affected by a change and generate relevant test cases. This not only saves time but also leads to more comprehensive testing.Pipeline OptimizationCI/CD pipelines often involve several stages, and it’s not always easy to determine which stages are causing bottlenecks. AI can analyze historical pipeline data to identify these bottlenecks and suggest improvements.For example, AI could identify a stage where tasks are frequently queued and suggest parallelizing the tasks to reduce wait times. Or it might find a stage where the build often fails and suggest improving the error detection in that stage.By leveraging AI, teams can take a data-driven approach to optimize their pipelines, focusing their efforts where they’ll have the most impact.Automating Routine TasksAI can also automate many routine tasks in CI/CD pipelines, such as setting up environments, managing dependencies, and configuring servers. This not only reduces the workload for developers but also ensures that these tasks are done consistently and correctly, reducing the potential for human error.Enhancing Decision MakingAI can provide actionable insights and recommendations to help teams make more informed decisions. For instance, by analyzing code changes and historical data, AI could predict the impact of a proposed change on the pipeline’s performance and reliability. This information can help teams make better decisions about when and how to implement changes.ConclusionThe integration of AI into CI/CD pipelines has the potential to bring about significant improvements in terms of efficiency, reliability, and speed. From predicting and detecting errors to optimizing testing and pipeline performance, AI opens up a myriad of possibilities that are set to revolutionize the way we manage and optimize CI/CD pipelines. By integrating AI into CI/CD processes, we can transform them from being reactive to proactive, allowing teams to address potential issues before they become problems." }, { "title": "The Future of Artificial Intelligence (AI) -Trends and Potential Impact", "url": "/posts/the-future-of-ai-trends-and-potential-impact/", "categories": "ai", "tags": "ai, artificial-intelligence, nlp, natural-language-processing, ml, machine-learning, deep-learning, nural-network", "date": "2023-06-09 00:00:00 +0200", "snippet": "Artificial Intelligence (AI) has rapidly evolved from a concept of science fiction to a transformative technology that is revolutionizing various industries. As AI continues to advance, it is cruci...", "content": "Artificial Intelligence (AI) has rapidly evolved from a concept of science fiction to a transformative technology that is revolutionizing various industries. As AI continues to advance, it is crucial to understand the current trends and potential impact it holds for businesses and society. In this blog post, we will explore the future of artificial intelligence, including key trends, emerging applications, and the potential benefits and challenges it presents.Accelerated Adoption across IndustriesAI has found applications in diverse industries such as healthcare, finance, manufacturing, retail, and more. As technology continues to advance, we can expect to see even wider adoption of AI-driven solutions. From personalized medicine and fraud detection to autonomous vehicles and smart cities, AI has the potential to transform every aspect of our lives.HealthcareThe healthcare industry has witnessed rapid adoption of AI-driven solutions. AI can analyze large volumes of medical data, including patient records, imaging results, and genomics data, to support diagnosis, treatment planning, and drug discovery. AI-powered algorithms can assist in early disease detection, personalized medicine, and predicting patient outcomes. Additionally, virtual health assistants and chatbots can provide 24/7 patient support, answer basic medical queries, and triage cases for healthcare professionals.FinanceIn the finance industry, AI has transformed numerous processes, ranging from fraud detection and risk assessment to algorithmic trading and customer service. Machine learning algorithms can analyze vast amounts of financial data in real-time to identify suspicious transactions, mitigate fraud, and enhance cybersecurity. AI-powered chatbots can provide personalized financial advice, assist with customer inquiries, and automate routine banking tasks. Moreover, AI-driven predictive analytics helps financial institutions make data-driven decisions for investment strategies and loan approvals.Manufacturing and Supply ChainAI has revolutionized manufacturing and supply chain operations. By leveraging AI technologies, manufacturers can optimize production processes, reduce defects, and improve product quality. AI-powered predictive maintenance systems can anticipate equipment failures and schedule maintenance proactively, minimizing downtime. Furthermore, AI-enabled demand forecasting and inventory management systems optimize supply chain operations, ensuring optimal stock levels and reducing costs.Retail and E-commerceAI has reshaped the retail and e-commerce landscape, providing personalized shopping experiences and enhancing customer engagement. AI-powered recommendation engines analyze customer data, browsing behavior, and purchase history to offer personalized product recommendations, increasing conversion rates. Virtual shopping assistants powered by natural language processing enable customers to interact and receive product recommendations through voice or chat interfaces. AI-driven inventory management systems optimize stock levels and supply chain efficiency, minimizing out-of-stock situations and improving customer satisfaction.Transportation and LogisticsAI plays a pivotal role in transforming transportation and logistics. Autonomous vehicles and drones powered by AI algorithms have the potential to revolutionize transportation and last-mile delivery. AI-powered route optimization algorithms can minimize fuel consumption, reduce delivery times, and optimize logistics operations. Additionally, AI-based traffic management systems help alleviate congestion and improve overall transportation efficiency.Energy and UtilitiesAI has been instrumental in optimizing energy consumption, predicting equipment failures, and enabling smarter energy management. AI-powered algorithms analyze energy consumption patterns, weather data, and historical records to optimize energy distribution, reduce wastage, and enable more efficient energy use. Smart grids powered by AI enable real-time monitoring, predictive maintenance, and demand-response systems, improving overall energy efficiency.Enhanced Automation and EfficiencyOne of the primary benefits of AI is its ability to automate mundane and repetitive tasks, freeing up human resources to focus on higher-value work. Machine learning algorithms and robotic process automation (RPA) enable businesses to streamline processes, improve efficiency, and reduce operational costs. As AI evolves, we can expect to see further automation across industries, leading to increased productivity and innovation.Business Process AutomationAI-powered automation solutions can streamline and optimize business processes. Robotic Process Automation (RPA) combines AI with automation capabilities to handle rule-based, repetitive tasks. RPA bots can perform data entry, generate reports, process invoices, and handle other routine tasks, freeing up human resources to focus on more strategic and value-added activities. By automating manual processes, organizations can achieve higher accuracy, reduce human errors, and increase operational efficiency.Customer Service and SupportAI has revolutionized customer service and support, enabling organizations to provide round-the-clock assistance and personalized experiences. Virtual assistants and chatbots powered by AI algorithms can handle customer inquiries, provide product recommendations, and assist with basic troubleshooting. Natural Language Processing (NLP) allows chatbots to understand and respond to customer queries in real-time, enhancing customer satisfaction and reducing response times. By automating customer service interactions, organizations can scale their support capabilities, provide instant support, and improve overall customer experience.Data Analysis and InsightsAI enables organizations to extract valuable insights from large volumes of data. Machine Learning algorithms can analyze vast datasets, identify patterns, and make predictions, enabling data-driven decision-making. AI-powered analytics platforms can automate data processing, generate reports, and provide actionable insights. This enhances operational efficiency by eliminating manual data analysis tasks, accelerating the decision-making process, and uncovering hidden insights that can drive business growth and optimization.Predictive MaintenanceAI-driven predictive maintenance systems leverage machine learning algorithms to anticipate equipment failures before they occur. By analyzing sensor data, historical maintenance records, and environmental conditions, AI algorithms can identify patterns and indicators of potential equipment malfunctions. This allows organizations to schedule maintenance proactively, reducing unplanned downtime and improving operational efficiency. Predictive maintenance can also optimize resource allocation, extend equipment lifespan, and reduce maintenance costs.Supply Chain OptimizationAI plays a significant role in optimizing supply chain operations. AI algorithms can analyze data from multiple sources, including demand forecasts, production schedules, and logistics data, to optimize inventory management, distribution routes, and warehouse operations. By automating supply chain processes, organizations can reduce costs, minimize inventory carrying costs, optimize delivery schedules, and enhance overall supply chain efficiency.Workflow AutomationAI-powered workflow automation tools streamline complex business workflows, ensuring smooth collaboration and efficient task management. These tools can automate task assignments, approvals, notifications, and document routing, eliminating manual hand-offs and reducing process bottlenecks. Workflow automation improves productivity, reduces human errors, and allows teams to focus on value-added activities, ultimately driving operational efficiency.Advancements in Natural Language Processing (NLP) and Conversational AINatural Language Processing (NLP) and Conversational AI have made significant strides in recent years, enabling more natural and human-like interactions between machines and humans. Virtual assistants, chatbots, and voice-controlled devices are becoming commonplace, transforming the way we interact with technology. In the future, NLP and Conversational AI will continue to improve, enhancing customer service, personalization, and accessibility.Voice Assistants and ChatbotsNLP advancements have led to the widespread adoption of voice assistants and chatbots. These AI-powered conversational interfaces enable users to interact with technology using natural language, making it more intuitive and user-friendly. Voice assistants like Amazon Alexa, Google Assistant, and Apple Siri can perform tasks, answer questions, and provide personalized recommendations. Chatbots are deployed across various industries for customer support, information retrieval, and transactional interactions. The advancements in NLP have improved the accuracy and contextual understanding of these conversational agents, making interactions more seamless and efficient.Sentiment Analysis and Emotion DetectionNLP techniques have made significant progress in understanding human emotions and sentiments expressed in text. Sentiment analysis algorithms can analyze social media posts, customer reviews, and feedback to gauge public opinion and sentiment towards products, services, or events. Emotion detection algorithms can identify emotions such as happiness, anger, or sadness expressed in written text or spoken words. These advancements in sentiment analysis and emotion detection enable organizations to gain insights into customer preferences, sentiment trends, and brand reputation, helping them make data-driven decisions and provide better customer experiences.Language Translation and Multilingual ProcessingNLP has made remarkable strides in language translation and multilingual processing. Machine translation models, such as Google Translate and Microsoft Translator, utilize deep learning techniques to translate text from one language to another with increasing accuracy. NLP algorithms also enable multilingual processing, where systems can understand and process multiple languages simultaneously. This has facilitated global communication, cross-cultural collaboration, and expanded access to information across language barriers.Contextual Understanding and Intent RecognitionAdvancements in NLP have improved the contextual understanding of language, enabling systems to interpret meaning and intent accurately. Contextual language models like BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer) have demonstrated exceptional language comprehension capabilities. These models can understand the context, disambiguate ambiguous phrases, and generate more coherent and contextually appropriate responses. Intent recognition techniques enable systems to understand user intentions and extract relevant information from user queries, enhancing the accuracy and effectiveness of conversational AI systems.Domain-Specific Language ModelsNLP advancements have led to the development of domain-specific language models. These models are trained on specific domains, such as legal, healthcare, or finance, and possess domain-specific knowledge and vocabulary. Domain-specific language models improve the accuracy and relevance of responses in specialized domains, enabling more effective and reliable conversational AI systems. They can understand industry-specific terminology, handle domain-specific queries, and provide tailored recommendations or solutions.Low-Resource and Multimodal NLPRecent advancements in NLP have focused on addressing challenges in low-resource languages and multimodal data processing. Low-resource languages often lack sufficient training data, making it difficult to develop accurate NLP models. However, transfer learning techniques and cross-lingual pre-training have enabled the transfer of knowledge from high-resource languages to low-resource languages, improving NLP capabilities for underrepresented languages. Multimodal NLP involves processing and understanding information from multiple modalities, such as text, images, and audio. This allows systems to analyze and respond to data that combines different types of information, enhancing the richness and versatility of conversational AI interactions.Ethical and Responsible AIAs AI becomes more prevalent, ethical considerations surrounding its use become increasingly important. Issues such as bias in AI algorithms, privacy concerns, and the impact on jobs and society need to be addressed. It is crucial for organizations and policymakers to prioritize ethical and responsible AI development, ensuring transparency, fairness, and accountability.Bias and FairnessAI algorithms are only as unbiased as the data they are trained on. Biases present in the training data can lead to biased outcomes and unfair treatment. It is crucial to address and mitigate biases in AI systems to ensure fairness and equal opportunities for all individuals, regardless of race, gender, or other protected characteristics. Transparency in the decision-making process of AI systems can help identify and rectify biases.Privacy and Data ProtectionAI systems often rely on vast amounts of data, raising concerns about privacy and data protection. Organizations must handle data responsibly, ensuring compliance with relevant privacy regulations and obtaining appropriate consent for data usage. Safeguards should be in place to protect sensitive personal information and prevent unauthorized access or misuse.Accountability and TransparencyAI algorithms can be complex and difficult to interpret. It is important to promote transparency and accountability in AI systems, especially when they make significant decisions that affect individuals’ lives. Organizations should strive to provide clear explanations of how AI systems arrive at their decisions and enable individuals to understand and challenge those decisions when necessary.Job Displacement and Workforce ImpactAs AI and automation technologies advance, there is concern about potential job displacement and its impact on the workforce. It is crucial to proactively address these concerns by promoting reskilling and upskilling programs to help individuals adapt to changing job requirements. Additionally, exploring ways to augment human work with AI systems can lead to new opportunities and increased productivity.Safety and SecurityAI systems can have significant real-world consequences, especially in domains like autonomous vehicles or healthcare. Ensuring the safety and security of AI systems is paramount to prevent accidents, malicious attacks, or unintended consequences. Robust testing, validation, and ongoing monitoring of AI systems are essential to maintain their reliability and mitigate risks.Social ImpactAI technologies have the potential to impact society at large. It is important to consider the broader societal implications of AI adoption and deployment. Engaging in open dialogue, involving diverse stakeholders, and considering the social impact of AI applications can help ensure that AI is used to benefit humanity as a whole.Continued Advancements in Deep Learning and Neural NetworksDeep learning, a subset of AI, has shown remarkable progress in areas such as image recognition, natural language processing, and speech synthesis. Neural networks have become more complex and sophisticated, enabling AI systems to learn and adapt from vast amounts of data. Ongoing research and advancements in deep learning techniques will unlock new possibilities for AI applications in the future.Architectural InnovationsThe development of new neural network architectures has significantly advanced deep learning. Convolutional Neural Networks (CNNs) have revolutionized computer vision tasks, enabling accurate image classification, object detection, and image segmentation. Recurrent Neural Networks (RNNs) and their variants, such as Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRUs), have improved sequence modeling and natural language processing capabilities. Transformer architectures, like the ones used in the groundbreaking BERT and GPT models, have enhanced language understanding and generation tasks. Continued architectural innovations have led to models with deeper layers, increased parameter efficiency, and improved performance on complex tasks.Transfer Learning and Pre-trainingTransfer learning has played a crucial role in advancing deep learning. Pre-trained models, such as ImageNet pre-trained CNNs or language models like BERT, have been trained on massive datasets and learned rich representations of visual and linguistic features. These pre-trained models can then be fine-tuned on specific tasks or domains with smaller datasets, leading to faster convergence and improved performance. Transfer learning has democratized AI by enabling researchers and practitioners to leverage pre-trained models and achieve state-of-the-art results with limited data and computational resources.Generative Models and Creative ApplicationsDeep learning has made significant strides in generative models, allowing machines to create content such as images, music, and text. Generative Adversarial Networks (GANs) have been successful in generating realistic images by pitting a generator against a discriminator in a game-like framework. Variational Autoencoders (VAEs) have enabled the generation of diverse and realistic data samples by learning the underlying distribution of the data. These generative models have found applications in various creative domains, including art, music, and content generation, pushing the boundaries of human-machine collaboration and creativity.Reinforcement Learning and RoboticsDeep learning has also made significant contributions to reinforcement learning (RL), an area focused on training agents to make sequential decisions through interactions with an environment. Deep RL algorithms, such as Deep Q-Networks (DQN) and Proximal Policy Optimization (PPO), have achieved breakthroughs in game-playing tasks, including AlphaGo and OpenAI Five. Deep RL has also been applied to robotics, enabling robots to learn complex tasks through trial and error, manipulation, and locomotion. These advancements in RL have the potential to revolutionize industries such as healthcare, manufacturing, and autonomous vehicles.Explainability and InterpretabilityAs deep learning models become more complex, understanding their decision-making processes has become increasingly important. Researchers have made progress in developing methods to interpret and explain deep learning models, aiming to uncover the factors influencing their predictions. Techniques such as attention mechanisms, saliency maps, and layer-wise relevance propagation help identify important features and highlight the model’s decision process. Explainable AI ensures transparency, builds trust, and enables users to understand and validate model outputs, especially in critical domains like healthcare and finance.Efficient Model Training and DeploymentEfficient model training and deployment have been significant focuses of research and development. Advancements in hardware acceleration, such as GPUs and specialized AI chips, have accelerated deep learning training and inference. Techniques like model quantization, knowledge distillation, and model compression have reduced model size and computational requirements without significant loss of performance. This allows for the deployment of deep learning models on resource-constrained devices like smartphones and edge devices, enabling real-time inferencing and bringing AI capabilities to the edge.ConclusionThe future of AI holds tremendous potential for innovation and transformation across industries. With accelerated adoption, enhanced automation, advancements in natural language processing, ethical considerations, and continued advancements in deep learning, we can expect AI to have a profound impact on the way we live and work. It is crucial to stay updated with the latest trends and embrace AI in a responsible and ethical manner to harness its full potential for positive change." }, { "title": "Terragrunt - Simplify and Scale Your Infrastructure with Ease", "url": "/posts/terragrunt-simplify-and-scale-your-infrastructure-with-ease/", "categories": "devops, terraform", "tags": "terragrunt, terraform, iac, infrastructure-as-code, dry, dont-repeat-yourselve", "date": "2023-06-05 00:00:00 +0200", "snippet": "In the world of infrastructure as code (IaC), managing complex infrastructure efficiently and consistently can be a daunting task. However, with the advent of tools like Terragrunt, this process ha...", "content": "In the world of infrastructure as code (IaC), managing complex infrastructure efficiently and consistently can be a daunting task. However, with the advent of tools like Terragrunt, this process has become significantly easier. Terragrunt, an open-source tool developed by Gruntwork, is designed to enhance and simplify the usage of Terraform, an industry-leading IaC tool. In this blog post, we will explore the benefits of Terragrunt and provide examples of how it can streamline your infrastructure management workflow.What is Terragrunt?Terragrunt is a thin wrapper tool that extends the functionality of Terraform, providing additional features and solving common challenges faced when working with Terraform code. It acts as a powerful orchestration layer, allowing for seamless configuration and deployment management of multiple Terraform modules, remote state storage, and cross-account deployments. Terragrunt simplifies the management of complex infrastructure environments and helps teams adhere to best practices.Benefits of TerragruntModular ConfigurationTerragrunt introduces a modular approach to Terraform configuration, allowing you to organize and reuse code effectively. It enables the creation of reusable modules that encapsulate infrastructure components, making it easier to manage and maintain configurations across multiple environments.DRY (Don’t Repeat Yourself)Terragrunt promotes code reusability and reduces duplication by enabling the extraction of common configuration elements into shared modules. This approach ensures consistency and reduces the likelihood of errors or inconsistencies across deployments.Remote State ManagementTerragrunt simplifies the management of Terraform’s remote state. It supports backend configurations like Amazon S3, Azure Blob Storage, or HashiCorp Consul, which provide a centralized and version-controlled store for Terraform state files. This feature helps maintain state integrity and enables seamless collaboration among team members.Dependency ManagementWith Terragrunt, you can define and manage dependencies between different Terraform modules. This feature ensures that dependent modules are deployed in the correct order, avoiding potential resource conflicts and increasing the reliability of your deployments.Encrypted VariablesTerragrunt offers a built-in encryption feature that allows you to securely store sensitive information such as access keys, secrets, or API tokens. By encrypting variables, you can protect sensitive data and ensure secure deployments without exposing confidential information in your configuration files.Practical ExampleSuppose you are working on a project that requires deploying a web application to different environments: development, staging, and production. Each environment requires its own set of resources, such as a virtual network, load balancer, and database.To start, you would create a folder structure for your project:my-project/├── environments/│ ├── dev/│ ├── stage/│ └── prod/├── modules/└── terragrunt.hclDefine Terraform Modules:In the modules/ directory, you would create reusable Terraform modules representing the infrastructure components required by your web application. For example, you might have modules for a virtual network, a load balancer, and a database.Configure Environments:In the environments/ directory, you would create separate folders for each environment (dev/, stage/, and prod/). Within each environment folder, you would define a terragrunt.hcl file to configure the environment-specific settings.For example, let’s look at the dev/terragrunt.hcl file:terraform { source = \"../modules\"}inputs = { environment = \"dev\" vpc_name = \"my-dev-vpc\" lb_name = \"dev-load-balancer\" db_name = \"dev-database\"}Here, we define the source as ../modules, indicating that the modules are located in the parent directory. We also specify environment-specific input variables, such as the environment name, VPC name, load balancer name, and database name.Similarly, you would create stage/terragrunt.hcl and prod/terragrunt.hcl files with the appropriate input variable values for each environment.Terragrunt Configuration:In the root of your project, you would have a terragrunt.hcl file that configures Terragrunt and defines common settings shared across all environments.include { path = \"environments/*\"}In this example, we use the include block to specify a wildcard path pattern (environments/*) to include all environment-specific configurations. This allows Terragrunt to automatically apply the configurations for each environment when executed.Deploying Infrastructure: To deploy the infrastructure for a specific environment, navigate to the respective environment directory and run Terragrunt commands.For instance, to deploy the development environment, you would go to the dev/ directory and execute Terragrunt commands:cd environments/devterragrunt initterragrunt planterragrunt applyTerragrunt will handle the initialization, planning, and applying of Terraform configurations using the specified input variables. It will also manage the state files for each environment, ensuring isolation and consistent deployments.You can repeat the same process for the staging and production environments by navigating to the corresponding directories (environments/stage and environments/prod) and running the Terragrunt commands.ConclusionBy leveraging Terragrunt, you can easily manage and deploy infrastructure across different environments while maintaining consistency and reducing duplication. Terragrunt’s modular approach, remote state management, and dependency handling simplify the management of complex infrastructure deployments." }, { "title": "Terraform Advanced Tips and Tricks", "url": "/posts/terraform-advanced-tips-and-tricks/", "categories": "devops, terraform", "tags": "terrafrom, iac, infrastructure-as-code, automation, tips, tricks", "date": "2023-05-24 00:00:00 +0200", "snippet": "Terraform is a powerful tool for managing Infrastructure as Code (IaC), allowing you to provision and manage resources efficiently. While the basics of Terraform are relatively straightforward, the...", "content": "Terraform is a powerful tool for managing Infrastructure as Code (IaC), allowing you to provision and manage resources efficiently. While the basics of Terraform are relatively straightforward, there are several advanced tips and tricks that can enhance your Terraform workflow and help you optimize your infrastructure automation. In this blog post, we will explore some advanced techniques and best practices to take your Terraform skills to the next level.Workspaces for Environment SeparationTerraform workspaces allow you to manage multiple environments (such as development, staging, and production) within a single Terraform configuration. By creating separate workspaces, you can maintain different state files and easily manage infrastructure changes across environments. Use the terraform workspace command to switch between workspaces and keep your configurations organized.# Create a new workspace for the development environmentterraform workspace new development# Switch to the staging workspaceterraform workspace select staging# List available workspacesterraform workspace listRemote State StorageStoring Terraform state files remotely provides better collaboration, version control, and security. Use remote state storage options such as Terraform Cloud, Amazon S3, or Azure Blob Storage to centrally store and manage your state files. This ensures consistency and avoids potential conflicts when working in a team or across different machines.terraform { backend \"s3\" { bucket = \"your-s3-bucket\" key = \"terraform.tfstate\" region = \"us-west-2\" }}Leverage Terraform ModulesTerraform modules enable code reuse and help standardize infrastructure patterns. Create reusable modules for common infrastructure components or application stacks. By abstracting away the complexity and providing configurable inputs and outputs, modules promote consistency, reduce duplication, and simplify maintenance.module \"vpc\" { source = \"terraform-aws-modules/vpc/aws\" version = \"3.0.0\" # Module inputs name = \"my-vpc\" cidr = \"10.0.0.0/16\"}Use Dynamic Block for Repeated Resource ConfigurationsThe dynamic block feature introduced in Terraform 0.12 allows you to generate repeated resource configurations dynamically. This is useful when defining multiple similar resources with different configurations. Instead of writing repetitive code, leverage the dynamic block to generate the desired resource configurations based on variables or data sources.resource \"aws_instance\" \"servers\" { count = var.instance_count dynamic \"block_device\" { for_each = var.block_devices content { device_name = block_device.value.device_name volume_size = block_device.value.volume_size } }}Data Sources for Cross-Resource CommunicationData sources in Terraform allow you to fetch information from external systems or existing resources and use that data during resource creation or configuration. Use data sources to retrieve data such as IP addresses, security group IDs, or DNS records from other resources or APIs. This facilitates better coordination and communication between resources.data \"aws_security_group\" \"existing\" { id = \"sg-0123456789abcdef0\"}resource \"aws_instance\" \"example\" { # ... vpc_security_group_ids = [ aws_security_group.existing.id, ] # ...}Conditional Resource Creation with Count and Conditional ExpressionsTerraform’s count feature and conditional expressions enable you to conditionally create or manage resources based on certain conditions or variables. This is useful when you want to dynamically control resource creation based on user inputs or environment-specific requirements. Use count and conditionals to create or destroy resources selectively.resource \"aws_instance\" \"example\" { count = var.create_instance ? 1 : 0 # ...}Use Terraform Providers and Provider ConfigurationsTake advantage of Terraform providers and provider configurations to interact with various cloud services, APIs, or third-party tools. Providers extend Terraform’s capabilities and enable seamless integration with different platforms. Customize provider configurations to fine-tune the behavior of the provider, such as adjusting API timeouts orprovider \"aws\" { region = \"us-west-2\" profile = \"my-aws-profile\" shared_credentials_file = \"/path/to/aws/credentials\"}Harness the Power of Terraform PluginsTerraform offers a rich ecosystem of community-built plugins that can extend its functionality and integrate with other tools or services. Explore the available plugins for Terraform and leverage them to simplify complex workflows, integrate with CI/CD pipelines, or automate additional tasks such as secrets management or infrastructure validation.provider \"terraform\" { required_version = \"&gt;= 1.0.0\" plugins { mycustomplugin = { source = \"mycompany/mycustomplugin\" version = \"1.0.0\" } }}# Use the custom plugin in your configurationresource \"mycustomplugin_resource\" \"example\" { # ...}Use Terraform Testing FrameworksAutomated testing is crucial for validating your Terraform configurations and ensuring that your infrastructure is deployed correctly. Explore testing frameworks like Terratest or Kitchen-Terraform to write and execute tests against your Terraform code. This helps catch potential issues or misconfigurations early in the development cycle.package testimport ( \"testing\" \"github.com/gruntwork-io/terratest/modules/terraform\" \"github.com/stretchr/testify/assert\")func TestTerraformInfrastructure(t *testing.T) { terraformOptions := &amp;terraform.Options{ TerraformDir: \"./infra\", } defer terraform.Destroy(t, terraformOptions) terraform.InitAndApply(t, terraformOptions) // Assertions to validate the infrastructure assert.Equal(t, \"expected-value\", terraform.Output(t, terraformOptions, \"output_variable\"))}" }, { "title": "Securing DevOps Pipelines - Best Practices for Robust Application Security", "url": "/posts/securing-devops-pipelines/", "categories": "devops, security", "tags": "devops, pipelines, security, sast, sca, iac, dast, ci-cd, owasp", "date": "2023-05-19 00:00:00 +0200", "snippet": "With the increasing adoption of DevOps practices, organizations are embracing faster software delivery cycles. However, this accelerated pace should not come at the cost of compromising application...", "content": "With the increasing adoption of DevOps practices, organizations are embracing faster software delivery cycles. However, this accelerated pace should not come at the cost of compromising application security. Securing DevOps pipelines is paramount to protect sensitive data, prevent security breaches, and maintain the trust of customers. In this blog post, we will explore best practices for robust application security in DevOps pipelines, covering key areas such as code security, vulnerability management, secure configurations, and automated security testing.Code SecuritySecure Coding PracticesPromote secure coding practices by implementing guidelines, training developers, and leveraging security-focused code reviews. Emphasize input validation, parameterized queries, and secure authentication and authorization mechanisms.Static Application Security Testing (SAST)Integrate SAST tools into the CI/CD pipeline to scan source code for potential security vulnerabilities, such as SQL injection, cross-site scripting (XSS), and insecure cryptographic implementations.Secrets ManagementAvoid hardcoding sensitive information in code or configuration files. Utilize secure secret management solutions, such as vaults or key management services, to store and retrieve secrets like API keys, passwords, or cryptographic keys.Vulnerability ManagementDependency ManagementRegularly update and patch dependencies, libraries, and frameworks to address known vulnerabilities. Employ dependency management tools to automate vulnerability scanning and tracking of dependencies.Software Composition Analysis (SCA)Incorporate SCA tools to identify vulnerabilities in third-party components and open-source libraries used in your applications. Monitor security advisories and promptly address any reported vulnerabilities.Secure ConfigurationsInfrastructure as Code (IaC) SecurityEmbed security best practices into infrastructure code by defining secure configurations for cloud resources, networks, and access controls. Leverage infrastructure scanning tools to validate and enforce secure configurations.Container SecurityImplement secure container configurations and leverage container security scanning tools to identify vulnerabilities and enforce security policies within containerized environments. Regularly update container images to include the latest security patches.Automated Security TestingDynamic Application Security Testing (DAST)Integrate DAST tools into the CI/CD pipeline to simulate real-world attacks against running applications. Perform automated security scans to identify common vulnerabilities and misconfigurations.Security Test AutomationAutomate security testing by leveraging frameworks and tools specifically designed for security testing, such as OWASP ZAP or Burp Suite. Incorporate security test cases into the regression testing suite to ensure ongoing security verification.Continuous Monitoring and Incident ResponseLog Management and MonitoringImplement centralized log management and real-time monitoring solutions to detect and respond to security events effectively. Employ security information and event management (SIEM) systems to correlate and analyze security-related logs.Incident Response PlanningDevelop an incident response plan that outlines procedures for identifying, containing, and mitigating security incidents. Regularly conduct incident response drills and ensure the involvement of relevant stakeholders.ConclusionIn the fast-paced world of DevOps, application security must be an integral part of the development and deployment process. By following best practices for code security, vulnerability management, secure configurations, and automated security testing, organizations can build robust DevOps pipelines that prioritize application security. By integrating security into every stage of the software development lifecycle, organizations can confidently deliver secure software while maintaining the trust of their users and safeguarding sensitive data." }, { "title": "Testing Docker Images - Tools and Best Practices", "url": "/posts/testing-docker-images-tools-and-best-practices/", "categories": "devops, docker", "tags": "docker, testing, bats, testinfra, serverspec, best-practices", "date": "2023-05-18 00:00:00 +0200", "snippet": "Testing is a crucial aspect of software development, and Docker images are no exception. Properly testing Docker images ensures their reliability, security, and compatibility across different envir...", "content": "Testing is a crucial aspect of software development, and Docker images are no exception. Properly testing Docker images ensures their reliability, security, and compatibility across different environments. In this blog post, we will explore the importance of testing Docker images and discuss various tools, including Bats, that can aid in this process.The Importance of Testing Docker ImagesTesting Docker images provides several benefits, such as:ReliabilityBy thoroughly testing Docker images, you can identify and fix issues before deploying them, ensuring a higher level of reliability and stability in production environments.CompatibilityTesting helps ensure that Docker images function correctly across different platforms, operating systems, and versions of dependencies, avoiding potential compatibility issues.SecurityDocker images often contain sensitive data or critical components. Testing helps identify security vulnerabilities and ensures that proper security measures are in place.Testing Strategies for Docker ImagesUnit TestingUnit tests focus on testing individual components or functionalities within the Docker image. It involves writing tests for each isolated part of the image, such as specific scripts, configuration files, or libraries.Integration TestingIntegration tests check the interactions between different components within the Docker image. This ensures that the image functions as a cohesive unit and that all integrated parts work together correctly.Acceptance TestingAcceptance tests verify whether the Docker image meets the requirements and expectations defined by stakeholders. These tests typically simulate real-world scenarios and use cases to ensure the image behaves as intended.Useful Tools &amp; Frameworks for Testing Docker ImagesBatsBats (Bash Automated Testing System) is a popular testing framework specifically designed for Bash scripts. It allows you to write simple and concise test cases using Bash syntax, making it ideal for testing Docker images. Bats provide an easy way to validate scripts, execute commands within the container, and assert expected outcomes.Here’s an example of a Bats test case for a Docker image:#!/usr/bin/env bats@test \"Ensure web server is running\" { run docker run -d --name myapp myapp-image run docker exec myapp curl localhost [ \"$status\" -eq 0 ] [ \"$output\" = \"Hello, World!\" ]}@test \"Verify database connection\" { run docker run -d --name db mydb-image run docker exec db psql --version [ \"$status\" -eq 0 ] [[ \"$output\" =~ \"psql (PostgreSQL)\" ]]}TestinfraTestinfra is a testing framework that allows you to write tests in Python to validate infrastructure and system state. It provides a high-level, Pythonic API for executing commands inside Docker containers and making assertions about their state. Testinfra complements Bats by providing a Python-based testing solution for Docker images.Here’s an example of using Testinfra to test a Docker image:import pytestfrom testinfra import DockerContainer# Define a fixture to create a Docker container@pytest.fixture(scope=\"module\")def docker_container(): # Run the Docker container with DockerContainer(\"myapp-image\") as container: yield container# Test to ensure the web server is runningdef test_web_server_running(docker_container): # Check if the web server process is running assert docker_container.process.get(comm=\"nginx\").args == \"/usr/sbin/nginx -g 'daemon off;'\" # Check if the web server is listening on port 80 assert docker_container.socket(\"tcp://0.0.0.0:80\").is_listening # Make an HTTP request to the web server response = docker_container.run(\"curl http://localhost\") assert response.stdout.strip() == \"Hello, World!\"# Test to verify the database connectiondef test_database_connection(docker_container): # Check if the database process is running assert docker_container.process.get(comm=\"postgres\").args == \"/usr/lib/postgresql/12/bin/postgres -D /var/lib/postgresql/12/main -c config_file=/etc/postgresql/12/main/postgresql.conf\" # Check if the database port is open and accepting connections assert docker_container.socket(\"tcp://0.0.0.0:5432\").is_listening # Connect to the database and execute a query response = docker_container.run(\"psql -U postgres -c 'SELECT version();'\") assert \"PostgreSQL\" in response.stdout# Run the testspytest.main()ServerspecServerspec is a Ruby-based testing framework for infrastructure testing. It enables you to write tests in Ruby to verify the state of your Docker image. Serverspec provides a DSL for defining assertions related to packages, files, services, ports, and other infrastructure aspects.Here’s an example of using Serverspec to test a Docker image:require 'serverspec'require 'docker'# Set the backend to Dockerset :backend, :dockerset :docker_url, ENV['DOCKER_HOST']set :docker_image, 'myapp-image'# Verify the web serverdescribe package('nginx') do it { should be_installed }enddescribe service('nginx') do it { should be_enabled } it { should be_running }enddescribe port(80) do it { should be_listening }enddescribe command('curl http://localhost') do its(:stdout) { should match /Hello, World!/ }end# Verify the databasedescribe package('postgresql') do it { should be_installed }enddescribe service('postgresql') do it { should be_enabled } it { should be_running }enddescribe port(5432) do it { should be_listening }enddescribe command('psql -U postgres -c \"SELECT version();\"') do its(:stdout) { should match /PostgreSQL/ }endIt’s worth mentioning that these are just a few examples, and there are many other testing frameworks and tools available for testing Docker images. The choice of framework or tool depends on your specific requirements, the programming languages and technologies used in your Docker image, and the types of tests you want to perform.Best Practices for Testing Docker ImagesAutomate TestsAutomating your tests ensures consistency and reduces human error. Use tools like Bats, Docker Compose, or Testinfra to automate your test suites, enabling easy execution and integration with your CI/CD pipeline.Test in IsolationWhen testing Docker images, isolate the testing environment from external dependencies to maintain consistency and avoid interference. Use Docker Compose to define any necessary dependencies within the test environment.Reproducible TestsEnsure that your tests consistently yield the same results, regardless of the testing environment or the execution context of your Docker images. This reliability enables you to have confidence in the accuracy and stability of your tests, allowing you to detect and address issues promptly, maintain the desired functionality of your applications, and facilitate seamless collaboration among team members. By implementing reproducible tests, you establish a solid foundation for maintaining the integrity and reliability of your Docker images throughout the software development lifecycle.ConclusionIn conclusion, testing Docker images is a crucial aspect of ensuring the reliability, functionality, and adherence to best practices in containerized applications. By incorporating appropriate testing frameworks and tools like Bats, Testinfra, and Serverspec, developers can validate the expected behavior of Docker images and detect any issues or deviations early in the development cycle. These testing frameworks offer different approaches and capabilities, allowing developers to write expressive tests, perform linting, and conduct integration testing.By investing in comprehensive testing for Docker images, developers can enhance the quality, stability, and security of their containerized applications, leading to more reliable and efficient deployments. With the help of these testing frameworks and a solid testing strategy, developers can confidently build and deploy Docker images with greater confidence and peace of mind." }, { "title": "Detect and Prevent Deployment Drift in Kubernetes", "url": "/posts/detect-and-prevent-deployment-drift-in-kubernetes/", "categories": "devops, kubernetes", "tags": "k8s, kubernetes, drift, terraform, helm, iac, infrastructure-as-code, gitops", "date": "2023-05-17 00:00:00 +0200", "snippet": "Kubernetes has revolutionized the way we deploy and manage applications, offering scalability, reliability, and ease of management. However, as your application evolves and deployment configuration...", "content": "Kubernetes has revolutionized the way we deploy and manage applications, offering scalability, reliability, and ease of management. However, as your application evolves and deployment configurations change, it’s crucial to be mindful of deployment drift. Deployment drift occurs when the actual state of your deployed application diverges from its desired state, leading to potential instability, performance issues, and security vulnerabilities. In this blog post, we will explore the concept of deployment drift, its causes, and provide practical strategies to detect and prevent it in your Kubernetes deployments.Understanding Deployment DriftDeployment drift is a phenomenon where the actual state of deployed resources (such as pods, services, and configurations) deviates from their desired state as defined in the deployment manifests. Several factors can contribute to deployment drift, including manual configuration changes, human errors, version mismatches, external interference, and unauthorized modifications. Detecting and addressing deployment drift is essential to maintain the reliability, performance, and security of your Kubernetes applications.Detecting Deployment DriftVersion ControlUtilize version control systems like Git to track and compare changes in deployment configurations. Regularly review and compare the version-controlled manifests against the deployed resources to identify any discrepancies.Configuration AuditingImplement configuration auditing tools and practices to analyze and compare the desired state against the actual state of your Kubernetes cluster. Tools like kube-score, kube-hunter, and kube-bench can help identify misconfigurations, security vulnerabilities, and deviations from best practices.Observability and MonitoringEmploy robust observability and monitoring practices to gain insights into the state of your Kubernetes cluster. Utilize metrics, logs, and tracing tools to detect anomalies, performance issues, and unexpected changes in resource utilization that may indicate deployment drift.CI/CD PipelinesIncorporate automated testing, validation, and verification steps into your CI/CD pipelines. Perform thorough checks against your deployment manifests and configurations before deploying changes to production environments.Preventing Deployment DriftInfrastructure as Code (IaC)Embrace Infrastructure as Code principles to define your Kubernetes deployments. Tools like Kubernetes YAML, Helm charts, or infrastructure provisioning tools such as Terraform provide declarative and reproducible ways to manage and maintain your application infrastructure, reducing the chances of deployment drift.Immutable InfrastructureAdopt the concept of immutable infrastructure, where changes to resources are made by creating new instances rather than modifying existing ones. By deploying new versions of applications instead of modifying running instances, you reduce the risk of drift and make rollbacks easier.Role-Based Access Control (RBAC)Implement RBAC policies to control and restrict access to your Kubernetes cluster. Grant permissions only to trusted users and regularly audit access controls to prevent unauthorized modifications that can lead to deployment drift.Automated Testing and ValidationCreate comprehensive test suites and validation processes to ensure the correctness of your deployment configurations. Leverage tools like Kubernetes Policy Controller, kubeval, kube-score, and conftest to perform automated checks on your manifests and enforce desired deployment practices.Change Management and DocumentationEstablish clear change management practices and document any modifications made to your deployment configurations. This ensures accountability, facilitates communication among team members, and helps identify the source of any deployment drift.Practical ExamplePreventing deployment drift in a Kubernetes environment often involves employing Infrastructure as Code principles. Let’s take a look at a practical example of utilizing Terraform to mitigate deployment drift effectively.TerraformImagine you have a Kubernetes cluster running in a cloud provider like AWS, and you’re managing your infrastructure using Terraform. You want to ensure that the deployed resources in your cluster match the desired state defined in your Terraform configuration. We’ll focus on detecting and preventing drift in the number of replicas for a specific deployment.Define the Desired StateIn your Terraform configuration, declare the desired state for the deployment, including the desired number of replicas. For example, you can have a resource block that defines a Kubernetes deployment:resource \"kubernetes_deployment\" \"example\" { metadata { name = \"example-deployment\" } spec { replicas = 3 # Other deployment specifications... }}Apply and Provision the InfrastructureUse Terraform to apply the configuration and provision the Kubernetes resources in your cluster. Run terraform apply to create or update the deployment.Monitor and Detect DriftAfter the deployment is provisioned, regularly monitor the cluster’s state to detect any drift. You can use a combination of Terraform and Kubernetes tools to compare the desired state with the actual state. Here’s an example of how you can accomplish this:# Use Terraform to output the current stateterraform output -json &gt; current_state.json# Use kubectl to retrieve the actual statekubectl get deployment example-deployment -o json &gt; actual_state.json# Compare the states using a diff tool (e.g., diff, jq)diff current_state.json actual_state.jsonBy comparing the Terraform output with the actual state retrieved from Kubernetes, you can identify discrepancies in the number of replicas or any other attributes of the deployment.Automate Drift DetectionTo make the drift detection process more efficient and automated, you can integrate the state comparison into a script or CI/CD pipeline. This script can be scheduled to run periodically or triggered after specific events. The script should execute the steps mentioned in the previous section (output Terraform state, retrieve actual state, and compare them).Drift PreventionIf drift is detected, it’s crucial to remediate it and bring the resources back into the desired state. Terraform provides the terraform plan command, which can be used to preview the changes required to reconcile the drift. You can follow these steps to bring the deployment back in line with the desired state:# Generate the plan to reconcile the driftterraform plan -out=drift-reconciliation.tfplan# Apply the plan to update the deploymentterraform apply \"drift-reconciliation.tfplan\"Terraform will identify the necessary changes to align the deployment’s replica count with the desired state and apply those changes to the cluster.By regularly monitoring, detecting, and leveraging Terraform’s infrastructure provisioning capabilities, you can ensure that the actual state of your Kubernetes deployments matches the desired state defined in your Terraform configuration, effectively detecting and preventing deployment drift.ConclusionDetecting and preventing deployment drift is crucial for maintaining the stability, performance, and security of your Kubernetes applications. By implementing robust monitoring practices, utilizing version control systems, embracing Infrastructure as Code principles, and adopting automated testing and validation, you can effectively mitigate deployment drift risks. Additionally, employing role-based access controls and enforcing change management practices will help prevent unauthorized modifications. By proactively addressing deployment drift, you can ensure your Kubernetes deployments remain reliable, scalable" }, { "title": "Progressive Delivery Techniques in DevOps", "url": "/posts/progressive-delivery-techniques-in-devops/", "categories": "devops", "tags": "devops, feature, flags, canary, a-b, blue-green, deployments, traffic-shifting", "date": "2023-05-16 00:00:00 +0200", "snippet": "In the world of software development and operations, DevOps has emerged as a paradigm shift, facilitating seamless collaboration between development and operations teams to deliver high-quality sof...", "content": "In the world of software development and operations, DevOps has emerged as a paradigm shift, facilitating seamless collaboration between development and operations teams to deliver high-quality software at scale. While the basic principles of DevOps, such as automation, continuous integration, and continuous deployment, are well-established, advanced techniques like progressive delivery have gained prominence.In this blog post, we will explore the concept of progressive delivery and its role in orchestrating continuous delivery in the DevOps ecosystem.Understanding Progressive DeliveryProgressive delivery is an approach that enables organizations to release software updates incrementally, allowing for controlled exposure to a subset of users or infrastructure. Unlike traditional release strategies where changes are pushed to all users simultaneously, progressive delivery allows for fine-grained control over the deployment process, minimizing risks and enabling faster feedback loops.Key Concepts and Techniques in Progressive DeliveryFeature FlagsFeature flags, also known as feature toggles, are a fundamental technique used in progressive delivery. By leveraging feature flags, developers can enable or disable specific features or functionalities at runtime. This capability enables teams to control the exposure of new features, conduct A/B testing, and perform canary releases.Canary ReleasesCanary releases involve deploying a new version of an application or service to a small subset of users or infrastructure. By monitoring the performance and stability of the canary release, organizations can assess the impact of the new version on a limited scale before rolling it out to the entire user base. Canary releases help mitigate risks associated with introducing new features or changes.A/B TestingA/B testing is a technique used to compare two versions of an application or service, commonly referred to as the control group and the experimental group. By dividing users into these groups and exposing each to different versions, organizations can gather data on user behavior, performance, and other key metrics. A/B testing provides valuable insights to inform decision-making regarding feature adoption and optimization.Blue-Green DeploymentsBlue-green deployments involve maintaining two identical production environments, referred to as blue and green. The blue environment represents the current stable version, while the green environment hosts the updated version. By routing traffic between the two environments, organizations can ensure a seamless transition from the blue environment to the green environment. Blue-green deployments minimize downtime and provide a rollback option if issues arise.Traffic ShiftingTraffic shifting is a technique used to gradually shift user traffic from one environment to another, such as from a canary release to the main production environment. This allows for controlled exposure and monitoring of new versions while ensuring a smooth transition for users. Traffic shifting can be based on predefined rules or automated metrics-driven strategies.Benefits of Progressive Delivery in DevOpsRisk MitigationProgressive delivery enables organizations to mitigate risks associated with software releases by gradually exposing changes to a limited audience. This approach facilitates early detection of issues, minimizing the impact on the entire user base.Faster Feedback LoopsBy releasing software updates incrementally, DevOps teams can gather feedback from real users, allowing them to identify and address issues promptly. This feedback loop improves the overall quality and usability of the software.Improved User ExperienceProgressive delivery techniques enable organizations to tailor experiences based on user segments. By deploying features selectively, teams can test new functionalities, measure user response, and make data-driven decisions to enhance the user experience.Increased Release ConfidenceBy leveraging canary releases, A/B testing, and traffic shifting, organizations can build confidence in their releases. Monitoring key metrics during these deployments allows teams to assess the stability, performance, and user satisfaction of new features before full-scale rollout.ConclusionAs the DevOps landscape evolves, progressive delivery techniques are gaining prominence as a powerful approach to orchestrating continuous delivery. By adopting feature flags, canary releases, A/B testing, blue-green deployments, and traffic shifting, organizations can minimize risks, obtain valuable user feedback, and optimize the user experience.Embracing progressive delivery empowers DevOps teams to accelerate their software delivery cycles, deliver higher-quality software, and stay ahead in today’s rapidly changing digital world." }, { "title": "Docker Multi-stage Builds", "url": "/posts/docker-multi-stage-builds/", "categories": "devops, docker", "tags": "docker, multi-stage-builds, image, layer, optimization", "date": "2023-05-15 00:00:00 +0200", "snippet": "Docker Multi-stage builds is a powerful feature that allows you to build more efficient and smaller Docker images by utilizing multiple stages in the build process. This feature was introduced in D...", "content": "Docker Multi-stage builds is a powerful feature that allows you to build more efficient and smaller Docker images by utilizing multiple stages in the build process. This feature was introduced in Docker 17.05 and has since become a popular practice for optimizing container images.In this blog post, we will explore what Docker multi-stage builds are, why anyone should use them, and the benefits they offer.What is Docker Multi-stage Builds?In traditional Docker builds, each step in the Dockerfile creates a new layer in the image. As a result, the final image can become large and bloated with unnecessary files and dependencies.Docker multi-stage builds, on the other hand, allow you to create multiple build stages within a single Dockerfile. Each stage can use a different base image and perform specific build steps. The final image only contains the artifacts from the final stage, which results in a smaller, more efficient image.In other words, multi-stage builds help you to create lean and optimized images by allowing you to perform multiple tasks in a single Dockerfile.Why use Docker Multi-stage Builds?There are several benefits to using Docker multi-stage builds:Smaller Image SizesOne of the primary benefits of multi-stage builds is that they result in smaller Docker images. This is because only the necessary files are included in the final stage, and intermediate build stages are discarded. This can lead to significant savings in terms of storage space and network bandwidth.Improved Build TimesMulti-stage builds can also help to improve build times. Separating the build process into multiple stages allows you to leverage caching to speed up the build process. Only the stages that have changed since the last build will be rebuilt, while the other stages can be retrieved from the cache.Better SecurityAnother benefit of multi-stage builds is improved security. By separating the build process into multiple stages, you can ensure that only the necessary dependencies are included in the final image. This can help to reduce the attack surface of the application.Practical ExampleLet’s take a look at a practical example of how to use multi-stage builds in a Dockerfile.Node.js ApplicationHere’s an example of a Dockerfile for a Node.js application that uses multi-stage builds:# First stage: build the Node.js appFROM node:14-alpine AS builderWORKDIR /appCOPY package*.json ./RUN npm installCOPY . .RUN npm run build# Second stage: run the app in a lightweight imageFROM node:14-alpineWORKDIR /appCOPY --from=builder /app/dist ./distCMD [\"npm\", \"start\"]In this example, the first stage uses the node:14-alpine image to build the Node.js application. It installs dependencies, builds the app, and produces a dist directory with the compiled code.The second stage uses the same base image, but only copies the dist directory from the first stage. This results in a much smaller image containing only the necessary files to run the app." }, { "title": "Ephemeral Builds - Empowering Efficient and Disposable Build Environments", "url": "/posts/ephemeral-builds-empowering-efficient-and-disposable-build-environments/", "categories": "devops, infrastructure", "tags": "ephemeral-builds, iac, infrastructure-as-code, automation, build-environments, disposable-builds", "date": "2023-05-14 00:00:00 +0200", "snippet": "In the world of software development, build environments play a crucial role in producing reliable and high-quality software. Traditionally, build environments have been long-lived and shared acros...", "content": "In the world of software development, build environments play a crucial role in producing reliable and high-quality software. Traditionally, build environments have been long-lived and shared across multiple developers. However, the concept of ephemeral builds has gained momentum, offering a fresh and disposable approach to building software. In this blog post, we will explore ephemeral builds, their benefits, and how they empower efficient and disposable build environments.Understanding Ephemeral BuildsEphemeral builds refer to the practice of creating temporary and short-lived build environments for each build process. Instead of relying on long-lived and shared build environments, ephemeral builds embrace the idea of starting with a clean slate for every build. Ephemeral build environments are created on-demand, serve a specific purpose, and are destroyed once the build process completes.Benefits of Ephemeral BuildsConsistency and ReproducibilityEphemeral builds ensure consistency and reproducibility by starting each build from a known state. With a fresh build environment for every build, you eliminate issues caused by conflicting dependencies, outdated configurations, or remnants from previous builds. This consistency leads to reliable and reproducible build outputs.Isolation and Dependency ManagementBy creating isolated build environments, ephemeral builds help manage dependencies efficiently. Each build can have its own set of dependencies, ensuring that different projects or versions do not interfere with one another. This isolation prevents version conflicts and enables parallel builds without compromising the stability of the overall system.Faster Feedback LoopsEphemeral builds promote faster feedback loops by reducing the time required to set up and tear down build environments. With on-demand creation of build environments, developers can quickly iterate on their code changes, run tests, and receive feedback in a shorter timeframe. This acceleration of feedback loops enhances developer productivity and accelerates software delivery.Security and Risk MitigationEphemeral builds contribute to improved security and risk mitigation. As each build environment starts from a clean state, the risk of security vulnerabilities, unauthorized access, or leakage of sensitive information is reduced. Ephemeral builds ensure that confidential or sensitive data from previous builds is not retained, minimizing the attack surface and potential risks.Scalability and Resource OptimizationEphemeral builds enable scalability and resource optimization. Build environments can be dynamically provisioned and de-provisioned as needed, ensuring efficient resource utilization. By leveraging cloud infrastructure or containerization technologies, ephemeral builds can scale horizontally to accommodate increased build demands, optimizing build processes and reducing costs.Best Practices for Ephemeral BuildsInfrastructure as Code (IaC)Adopt Infrastructure as Code (IaC) practices to define and manage build environment configurations. Tools like Docker, Kubernetes, or HashiCorp Packer allow you to version control and automate the creation of ephemeral build environments.Orchestration and AutomationUtilize build orchestration tools such as Jenkins or Buildkite CI/CD to automate the process of creating and tearing down ephemeral build environments. These tools enable seamless integration with version control systems and provide mechanisms for defining and executing build pipelines.Artifact CachingImplement artifact caching mechanisms to optimize build times. By caching build dependencies or intermediate build artifacts, subsequent builds can leverage these cached resources, further accelerating the build process while maintaining consistency and reproducibility.Test and Validation StrategiesEmploy comprehensive testing and validation strategies for your build environments. Include unit tests, integration tests, and security scans to ensure the integrity and quality of your build processes. Automated testing as part of the build pipeline helps catch issues early and maintain the reliability of ephemeral builds.Continuous ImprovementContinuously evaluate and improve your ephemeral build processes. Analyze build metrics, identify bottlenecks, and optimize build configurations and dependencies. Regularly update build environment images or templates to include the latest patches and improvements.ConclusionEphemeral builds offer a fresh and disposable approach to building software, enabling consistent, reproducible, and efficient build environments. By embracing ephemeral builds, organizations can achieve faster feedback loops, improved security, scalability, and resource optimization. Implementing best practices such as Infrastructure as Code, automation, artifact caching, and continuous improvement helps unleash the full potential of ephemeral builds and empowers teams to deliver high-quality software with efficiency and confidence." }, { "title": "10 Useful CLI tools for DevOps Practitioners with practical examples", "url": "/posts/10-useful-cli-tools-for-devops-practitioners-with-practical-examples/", "categories": "devops, tools", "tags": "devops, cli-tools, command-line, terminal, homebrew, aws, kubernets, http, json, fuzzy-search, github", "date": "2023-04-24 00:00:00 +0200", "snippet": "As a DevOps practitioner, you’re probably always on the lookout for ways to improve your workflow and be more productive.One way to do this is by using command-line tools or CLI tools. CLI tools ar...", "content": "As a DevOps practitioner, you’re probably always on the lookout for ways to improve your workflow and be more productive.One way to do this is by using command-line tools or CLI tools. CLI tools are programs that run in the terminal or command prompt, and they can help you automate repetitive tasks, manage your infrastructure, and more.In this post, we’ll explore 10 of my favorite CLI tools: Homebrew AWS CLI k9s stern kubectx &amp; kubens xh jq jqp fzf ghHomebrewHomebrew, or brew for short, is a package manager for macOS that allows you to install and manage CLI tools and other software packages. It has a simple and user-friendly CLI that allows you to search for, install, and update packages with just a few commands.Using brew, you can install and manage CLI tools and other dependencies, including those mentioned in this post.Practical Examplebrew install &lt;package-name&gt;orbrew install --cask &lt;application-name&gt;AWS CLIIf you’re using Amazon Web Services (AWS), the aws CLI is a must-have tool. It allows you to manage your AWS resources using a CLI, including EC2 instances, S3 buckets, and more.The AWS CLI is an essential tool if you need to manage AWS resources programmatically.Practical ExampleCopy a S3 resource to a local path:aws s3 cp s3://&lt;resource-name&gt; &lt;local-path&gt;k9sk9s is a CLI tool for Kubernetes that provides a terminal-based UI for managing and monitoring your Kubernetes clusters. It allows you to view resources, logs, and metrics for your clusters, and it has a built-in search and filtering capabilities.With k9s, you can easily manage your Kubernetes clusters without leaving the terminal.Practical ExampleRun k9s for a specific namespace:k9s -n &lt;namespace&gt;sternstern is another powerful CLI tool for Kubernetes that allows you to tail logs from multiple pods at once. It has a simple and intuitive CLI that allows you to quickly switch between pods and namespaces, and it supports filtering and highlighting based on keywords.stern is a great tool for debugging and monitoring your Kubernetes applications, and it can save you a lot of time compared to manually searching through logs.Practical ExampleShow logs for specific pods:stern &lt;regex-or-pod-name&gt;kubectx &amp; kubenskubectx is a CLI tool for managing and switching between Kubernetes contexts. It allows you to easily switch between different Kubernetes clusters with a simple command, without having to remember long and complex commands.Simply run the kubectx command and select a context from the list.kubens is another useful CLI tool for Kubernetes that works alongside kubectx. With kubens, you can quickly switch between different namespaces, and avoid mistakes caused by accidentally running commands on the wrong namespace.xhxh (formerly known as httpie) is a CLI tool for making HTTP requests that provides a more user-friendly and intuitive interface than traditional command-line tools such as curl.It supports a variety of HTTP features such as JSON and form data, file uploads, authentication, and more, and it has a syntax that’s easy to read and understand. xh is a great tool if you need to interact with web APIs and test HTTP endpoints.With xh, you can quickly and easily make HTTP requests from the command line, without having to remember complex syntax or multiple options.Practical ExampleQuery a web API endpoint using request parameters:xh httpbin.org/json id==5Output:HTTP/1.1 200 OKAccess-Control-Allow-Credentials: trueAccess-Control-Allow-Origin: *Connection: keep-aliveContent-Length: 429Content-Type: application/jsonDate: Mon, 24 Apr 2023 18:22:26 GMTServer: gunicorn/19.9.0{ \"slideshow\": { \"author\": \"Yours Truly\", \"date\": \"date of publication\", \"slides\": [ { \"title\": \"Wake up to WonderWidgets!\", \"type\": \"all\" }, { \"items\": [ \"Why &lt;em&gt;WonderWidgets&lt;/em&gt; are great\", \"Who &lt;em&gt;buys&lt;/em&gt; WonderWidgets\" ], \"title\": \"Overview\", \"type\": \"all\" } ], \"title\": \"Sample Slide Show\" }}jqjq is a lightweight and powerful command-line tool for parsing JSON data. It allows you to filter, transform, and format JSON data using a simple syntax. jq is an essential tool if you work with JSON data regularly.Practical ExampleFilter aws s3api JSON response to determine whether accelerated transfer is enabled on a specific bucket:aws s3api get-bucket-accelerate-configuration --bucket &lt;bucket-name&gt; | jq '.Status' -r || trueOutput:Enabledjqpjqp is a command-line tool for parsing and querying JSON data. It allows you to filter and transform JSON data using a SQL-like query language.jqp is similar to the previously mentioned jq tool, but it has some additional features, such as support for joins, aggregation, and subqueries. jqp is a great tool if you work with complex JSON data and need a powerful and flexible way to extract and manipulate data.With jqp, you can quickly extract and transform data from JSON files or APIs, and integrate it with your other CLI tools and workflows.Practical ExampleQuery a web api enpoint and pipe the JSON response with jqp:xh httpbin.org/json id==5 | jqpFrom jqp you can then query the JSON response, for example:.slideshow.slides[0].titleOutput:\"Wake up to WonderWidgets!\"fzffzf is a command-line fuzzy finder tool that allows you to quickly search for files, directories, and other items in your file system. It provides a fast and interactive way to search and navigate your file system, with support for keyboard shortcuts and customizable search options.fzf is a great tool when you work with large and complex file systems and want a more efficient way to navigate and search them.With fzf, you can quickly search for files, directories, and other items in your file system, and open them in your preferred editor or tool with just a few keystrokes.Practical ExampleCheckout out a recent branch using Git and fzf:git branch --sort=-committerdate | fzf --header \"Checkout Recent Branch\" --preview \"git diff --color=always {1}\" | xargs git checkoutRunning the above command will present a list of recent Git branches that you can quickly check out. Pro Tip: Create an alias like gbr using the above command to avoid having to type out the full command every time.ghgh is a CLI tool developed by GitHub that allows you to interact with GitHub from the command line. It provides a convenient way to create, view, and manage issues and pull requests, as well as perform other common GitHub tasks such as cloning repositories, creating releases, and managing workflows.With gh, you can quickly perform common tasks without having to switch between the command line and the GitHub web interface.Practical ExampleCreate a Pull Request on GitHub with main as the base branch:gh pr create -B mainRunning the above command will open the GitHub web interface in your browser and start creating the Pull Request." }, { "title": "Immutable Infrastructure - Building Resilient and Scalable Systems", "url": "/posts/immutable-infrastucture-building-resilient-and-scalable-systems/", "categories": "devops, infrastructure", "tags": "immutalbe, infrastructure, iac, infrastructure-as-code, terraform, cloud, kubernetes, docker", "date": "2023-03-15 00:00:00 +0200", "snippet": "In the quest for reliable and scalable infrastructure, Immutable Infrastructure has emerged as a powerful concept. By treating infrastructure as immutable, you can achieve greater resilience, scala...", "content": "In the quest for reliable and scalable infrastructure, Immutable Infrastructure has emerged as a powerful concept. By treating infrastructure as immutable, you can achieve greater resilience, scalability, and security. In this blog post, we will explore the concept of Immutable Infrastructure, its benefits, and best practices for building resilient and scalable systems.Understanding Immutable InfrastructureImmutable Infrastructure refers to the practice of creating and deploying infrastructure components, such as servers, containers, and virtual machines, as immutable artifacts. Instead of modifying existing infrastructure, immutable infrastructure promotes the creation of new instances whenever changes are required. Immutable infrastructure is designed to be disposable, with changes made by replacing existing instances with updated ones, rather than modifying them in-place.Key Benefits of Immutable InfrastructureImproved System Stability and ReliabilityWith immutable infrastructure, changes are applied by deploying entirely new instances rather than modifying existing ones. This approach minimizes the risk of configuration drift, dependency issues, and inconsistencies that can lead to system failures or performance degradation. Immutable infrastructure promotes stability and ensures that each deployment starts from a known, clean state.Enhanced ScalabilityImmutable infrastructure supports effortless horizontal scaling. By creating new instances to handle increased demand, organizations can easily scale their systems without worrying about managing complex state modifications. Scaling becomes as simple as provisioning new instances and distributing the workload across them.Simplified Rollbacks and RecoverySince immutable infrastructure maintains a history of previous instances, rolling back to a previous version becomes straightforward. In case of failures or issues, organizations can quickly revert to a known-good state by replacing the problematic instances with previous versions. This capability enhances system recovery and reduces downtime.Security and ComplianceImmutable infrastructure reduces the attack surface and enhances security. By creating new instances instead of modifying existing ones, the risk of unauthorized access, configuration errors, and security vulnerabilities is minimized. Immutable infrastructure also facilitates compliance by providing a consistent and auditable deployment process.Best Practices for Building Immutable InfrastructureInfrastructure as Code (IaC)Adopt Infrastructure as Code (IaC) practices to define and manage infrastructure configurations as code. Tools like Terraform or AWS CloudFormation allow you to version control, test, and automate the creation of infrastructure artifacts.Image and Artifact ManagementLeverage containerization or virtual machine images to create immutable artifacts. Use container registries or artifact repositories to store and manage these images. Ensure that images are versioned and properly tagged for easy tracking and deployment.Configuration ManagementSeparate application configuration from infrastructure configuration. Store application-specific configurations in externalized configuration management systems like Consul or etcd. This allows you to update application configurations without modifying the underlying infrastructure.Immutable DeploymentsFollow immutable deployment patterns by creating new instances for each deployment. Automate the process of provisioning new instances, deploying applications, and routing traffic to the new instances. Implement blue-green deployments or canary releases to ensure smooth transitions and minimize the impact of deployments.Automated Testing and ValidationImplement comprehensive testing and validation strategies for your infrastructure code and artifacts. Use unit tests, integration tests, and security scans to ensure the integrity and quality of your infrastructure components. Automate these tests as part of your CI/CD pipeline to catch issues early.ConclusionImmutable Infrastructure offers significant advantages in terms of stability, scalability, and security. By adopting the principles of immutable infrastructure and leveraging Infrastructure as Code practices, organizations can build resilient and scalable systems that are easy to manage and recover. The shift towards immutable infrastructure empowers DevOps teams to deliver reliable and efficient infrastructure, enabling organizations to meet the demands of today’s dynamic and evolving digital landscape." }, { "title": "CI/CD with GitHub Actions", "url": "/posts/ci-cd-with-github-actions/", "categories": "devops, ci-cd", "tags": "ci-cd, github-actions, automation, pipeline", "date": "2021-05-24 00:00:00 +0200", "snippet": "Today’s approach to software development requires software to be delivered frequently, reliably, and efficiently. We can achieve this by automating the process using CI/CD pipelines.Continuous Inte...", "content": "Today’s approach to software development requires software to be delivered frequently, reliably, and efficiently. We can achieve this by automating the process using CI/CD pipelines.Continuous Integration (CI) means using automation tools to build, test, and merge code seamlessly. This is to ensure that code coming from different developers and components is free of errors. Automated tests help spot bugs at the early stage of software development and fix them immediately.Continuous Delivery (CD) is the practice of releasing software in short cycles, with greater speed and frequency. New code is released in smaller batches, even a few times a day. This makes it more bug-resistant and easier to manage.GitHub Actions makes it easy to automate all your software workflows, now with world-class CI/CD. Build, test, and deploy your code right from GitHub.GitHub Actions key featuresGitHub Actions offer some really nice features making it a viable option for projects of any size. Best of all, it is free (with limits), even on the free plan.Below are some of the key features offered by GitHub Actions: Hosted runners for every major OS hosted by GitHub Self-hosted runners Matrix builds to simultaneously test across multiple operating systems Support for most of the popular programming languages Live logs Built in secret store Multi-container testing Community-powered workflowsCreating a workflow aka ActionFolder structureCreating a new workflow requires that you create a new file with the following folder structure (within the root folder of your project):.github/workflows/my-cicd-workflow.yml Folder and file names starting with a dot (.) are hidden. When committing and pushing your code changes, GitHub will look for your workflows within this folder structure and automatically run them accordingly.YAML workflow definitionGitHub Actions uses YAML to define workflow definitions.For our example, we will have a look at a workflow for creating a containerized microservice written in Go:name: CIon: push: branches: - developjobs: build-and-publish-develop: runs-on: ubuntu-latest if: github.ref == 'refs/heads/develop' steps: - uses: actions/checkout@v2 - name: Setting up Go uses: actions/setup-go@v2 with: go-version: 1.15 - name: Build app run: go build -v ./... - name: Run unit tests run: go test -v ./... - name: Build and publish docker image uses: VaultVulp/gp-docker-action@1.1.7 with: github-token: $ image-name: my-microservice image-tag: developLet’s start by dissecting the workflow file and see what makes up a typical workflow:The name of your worflowname: CIThis translates to the name shown on GitHub, under the Actions menu:TriggersTriggers that will run our workflow. Here we tell GitHub to run our workflow as soon as we push code changes to the develop branch of our repository.on: push: branches: - developJobsJobs that will execute as soon as our triggers get triggered. Jobs run in sequential order and will fail the entire workflow as soon as one of them fails. Only once all the jobs successfully run does the workflow completes.jobs: build-and-publish-develop: runs-on: ubuntu-latest if: github.ref == 'refs/heads/develop' steps: - uses: actions/checkout@v2 - name: Setting up Go uses: actions/setup-go@v2 with: go-version: 1.15 - name: Build app run: go build -v ./... - name: Run unit tests run: go test -v ./... - name: Build and publish docker image uses: VaultVulp/gp-docker-action@1.1.7 with: github-token: $ image-name: my-microservice image-tag: developRunnersTo run your jobs, you need to specify the runner it will run on. This is done by setting runs-on.In our example, we are setting up our runner to use the latest version of Ubuntu:runs-on: ubuntu-latestStepsJobs consist of multiple steps which in turn can run other actions.Below we have a step that uses a community-powered action called actions/setup-go@v2 that sets up a go environment for us: steps: - uses: actions/checkout@v2 - name: Setting up Go uses: actions/setup-go@v2 with: go-version: 1.15Similarly, we also have steps to automatically build our app, run the unit tests, and publish the docker image on GitHub: - name: Build app run: go build -v ./... - name: Run unit tests run: go test -v ./... - name: Build and publish docker image uses: VaultVulp/gp-docker-action@1.1.7 with: github-token: $ image-name: tilliopos-data-service image-tag: developOn GitHub, this translates to:Community-powered actionsAnother great feature of GitHub Actions is the extensive collection of community-powered, open-source actions you can use. These are great time savers, and more often than not, you will find that somebody has already published an action that you integrate into your workflow.In just this example alone, I was able to use two different community-powered actions, one for setting up a Go environment: actions/setup-go@v2 and the other for building and publishing a Docker image to GitHub VaultVulp/gp-docker-action.Workflows in actionNow that we have created our workflow definition, it is time to commit and push our code changes to GitHub:git add .git commit -m \"Adding GitHub workflow\"git push origin -u developGitHub will detect that we have a workflow definition in .github/workflows/ and will automatically start to run it every time we push code changes.Navigate to the Actions menu on GitHub and see your workflow in action:ConlusionIf you haven’t yet considered automating your integration and deployment processes, you really should! And with GitHub Actions, this is now easier than ever.What I like about GitHub Actions is how low the barrier to entry is. Getting started is really easy with minimal setup required and will save you a lot of time, money, effort, and frustration in the long run once set up." }, { "title": "SwiftUI - Creating custom Xcode library items", "url": "/posts/swiftui-creating-custom-xcode-library-items/", "categories": "swiftui", "tags": "swift, swiftui, xcode, library", "date": "2020-12-08 00:00:00 +0200", "snippet": "The Xcode library is a great way to discover and learn about available SwiftUI views and modifiers and allows you to conveniently drag and drop them to the Xcode Previews canvas without writing any...", "content": "The Xcode library is a great way to discover and learn about available SwiftUI views and modifiers and allows you to conveniently drag and drop them to the Xcode Previews canvas without writing any code.New in Xcode 12 and Swift 5.3, is the ability to extend the Xcode Library with your custom views and modifiers making it easier to discover and reuse views within your app or Swift Packages.Adding viewsLet’s take a look at how we can extend the Xcode library with our custom views that we can then add to the Preview canvas, just like those provided by the standard SwiftUI library.Structuring your viewsLet’s say we have a custom rounded button component that we’d like to reuse across several different views. We start by building out our custom view like any other SwiftUI view:struct RoundedButton: View { let title: String let backgroundColor: Color let foregroundColor: Color let cornerRadius: CGFloat let action: () -&gt; Void var body: some View { Button(title) { action() } .padding() .background(backgroundColor) .foregroundColor(foregroundColor) .cornerRadius(cornerRadius) .font(.title) }}It is crucial to pay attention to the structure of our code to make it reusable in different contexts. From the example above, instead of hardcoding all the modifier values, as usual, we declare properties and assign them to the modifiers. By doing so, we can now configure and use our custom button with different configurations throughout our UI.Creating an Xcode LibraryItem (for views)The first step is to create a type that conforms to the LibraryContentProvider protocol:struct XcodeLibraryContent: LibraryContentProvider }The LibraryContentProvider protocol has two requirements: views property - to extend the views Xcode library modifiers function - to extend the modifiers Xcode libraryBoth return an array of library items ([LibraryItem]) to extend the Xcode library. Xcode will automatically scan our source code for types conforming to the LibraryContentProvider protocol and add them to the library without having to build or run our code.In this section, we will focus on extending the views section of the Xcode library by creating a new LibraryItem and assigning it to the views array:@LibraryContentBuilder public var views: [LibraryItem] { LibraryItem(RoundedButton(title: \"Button\", backgroundColor: .red, foregroundColor: .white, cornerRadius: 12, action: { }), visible: true, title: \"Rounded Button\", category: .control) }The first argument when initializing the LibraryItem, is the view we want to add to the library, in this case, our custom button RoundedButton. Here we pass in the default data that will be used once the view is added to the Previews canvas.The default values you specify is really up to you and serves to act as a starting point that can be customized based on the insertion context. @LibraryContentBuilder is a new function builder for generating arrays of LibraryItem instances without requiring full array literal syntax.And just like that, we have now extended the Xcode library with our custom rounded button view.Click on the “+” icon in Xcode to open the library (⇧ + ⌘ + L) and make sure you have “Show the views library” selected.Our custom rounded button is now showing up together with all the other system provided views. Remember, views do not have to correspond to library items one-to-one, meaning that you can have multiple library items representing views in different configurations.Adding view modifiersJust like views, we can also extend the Xcode library with custom view modifiers and is extremely useful when you want to group several modifiers into a single modifier that you can reuse with different views.Extensions vs. View ModifiersWith extensions, we can encapsulate common layout and styling logic that can be reused across many views, they do however have limitations.Using view modifiers, we can leverage @State and other View-related behavior and apply this behavior to arbitrary views.Let’s create an extension on View for a specific style variation of our custom rounded button:extension View { func rotated3DButtonStyle(shadowColor: Color) -&gt; some View { self .rotationEffect(.degrees(-10)) .rotation3DEffect(.degrees(14), axis: (x: 1, y: 10, z: 0)) .shadow(color: shadowColor, radius: 20, x: 0, y: 0) }}Creating an Xcode LibraryItem (for view modifiers)To add our custom view modifier to the Xcode library, we yet again need to create a new LibraryItem and return it as an array in the modifiers function of the LibraryContentProvider protocol:@LibraryContentBuilder public func modifiers(base: AnyView) -&gt; [LibraryItem] { LibraryItem(base.rotated3DButtonStyle(shadowColor: .blue)) }Here the first argument when initializing the LibraryItem, is our base. The base enables Xcode to figure out which part is the modifier and which part is the view it modifies.So, when applying a modifier to Image, the base will be Image. In our case, given the example above, we’re applying a modifier to View, so our base will be AnyView.Again, just like that, we have now also extended the Xcode library with our custom view modifier.Click on the “+” icon in Xcode to open the library (⇧ + ⌘ + L). This time around, make sure you have “Show the Modifiers library” selected.Our custom view modifier is now showing up together with all the other system-provided view modifiers.What about Swift Packages?Since Xcode automatically scans all your source code files for LibraryContentProvider code, including all dependencies you might have, it also works great with Swift Packages.SummaryThe Xcode library is an easy way to discover available SwiftUI views and modifiers that we can easily add to the Xcode Previews canvas without writing any code, enabling rich visual editing of our apps.Extending the Xcode library with our own views and view modifiers is very easy and requires little effort." }, { "title": "Swift Packages - New in Swift 5.3", "url": "/posts/swift-packages-new-in-swift-5-3/", "categories": "swift", "tags": "swift, package, spm, swift-package-manager", "date": "2020-11-19 00:00:00 +0200", "snippet": "Following up on one of my previous article, it is time to revisit Swift Packages and discover what is new in Swift 5.3 and Xcode 12 and how we can take advantage of all the new features.SwiftUI Pre...", "content": "Following up on one of my previous article, it is time to revisit Swift Packages and discover what is new in Swift 5.3 and Xcode 12 and how we can take advantage of all the new features.SwiftUI PreviewsWith SwiftUI becoming increasingly more popular it only makes sense that we now have proper SwiftUI preview support for packages starting from Xcode 12.Before Xcode 12, rendering SwiftUI previews for packages required a couple of workarounds which involves creating an Xcode workspace and adding a client application, etc. Luckily, none of these workarounds are needed anymore.Updating from earlier versionsA few essential things to keep in mind when updating your existing Swift Packages created with earlier versions of Swift or Xcode.Ensure that you update your package manifest to use the latest required tools version:swift-tools-version:5.3Remember to specifiy the platforms and their versions your package supports:platforms: [.iOS(.v13), .macOS(.v10_15), .watchOS(.v6), .tvOS(.v13)] When adding a SwiftUI view to your package in Xcode 12 you may encounter the following error when trying to render the preview: SchemeBuildError: Failed to build the scheme \"\". To resolve the error, simply remove .testTarget entry in your package manifest file (Package.swift). This appears to be a bug in Xcode 12.Package ResoucesAlso new in Swift 5.3 and Xcode 12, is the ability to add resources such as images, storyboards, asset catalogs, localized resources, and much more.This was one of the key features that was missing in earlier versions of Swift and I’m really glad to see it finally being implemented.Adding resources to a packageAdding resources to a package is pretty straightforward - simply create a new file or add an existing file in your package folder under your package’s Sources folder:Resource file typesFiles in a package are processed according to their type, as indicated by the filename suffix.Clear purpose resource files like .xcassets, .storyboard, .xib, .nib, .xcdatamodel, xcmappingmodel, etc. has clear purpose and Xcode knows what to do with them. These clear purpose resource files can simply be added without any additional action required.Other types of resource files can have a variety of purposes and for those, we need to declare their intent in the package manifest.Declaring the resource’s intentLet’s have a look at the following package manifest:// swift-tools-version:5.3import PackageDescriptionlet package = Package( name: \"SwiftPackageExample\", defaultLocalization: \"en\", platforms: [ .iOS(.v13), .macOS(.v10_15), .watchOS(.v6), .tvOS(.v13) ], products: [ .library( name: \"SwiftPackageExample\", targets: [\"SwiftPackageExample\"]), ], targets: [ .target( name: \"SwiftPackageExample\", excludes: [\"Release Notes.txt\"], resources: [ .process(\"Image.png\"), .process(\"StaticData.json\"), .copy(\"Zip Codes\")] ) ])Adding filesBecause Image.png does not have a clear purpose, we need to declare the file’s intent in the package manifest. We do so by adding a .process action:resources: [.process(\"Image.png\")] Most resources should use the .process action for it to be transformed as appropriate at build time. The type of processing that will take place greatly depends on the platform for which the package is built.Adding foldersYou can also add folders using the .copy action:resources: [.copy(\"Zip Codes\")] Using the .copy action will copy the entire folder while also preserving the folder structure.Excluding files and foldersSome files or folders might only be needed during development and can be excluded:excludes: [\"Release Notes.txt\"] The same syntax above also applies for excluding folders.A couple of notes regarding resource processingProcessing: The .process action uses the built-in rules as appropriate for the platform Recommend choice except in special circumstances Falls back to .copy if the file type is unknown or when no special processing is needed For folders, the processing rules are applied recursively to all files under the directoryCopying: .copy makes a verbatim copy regardless of the file type No transformation takes place, good for source files, etc. Copies the whole directory as-is, preserving the folder structureHow are resources linked and bundled together with your app?Source files of a package get compiled into a code module and then linked into the app. Resources of a package get processed into a resource bundle and then gets embedded in the app.The resource bundle becomes part of the main app bundle and therefore will be available at runtime.For unbundled applications like command-line applications, resource bundles need to be installed alongside the application/tool.Accessing package resources from codeAccessing package resources from code is equally straightforward and is mainly done using Foundation’s Bundle API. Accessing resources is the same on all platforms that support Swift Packages and is independent of built artifacts.Accessing a file using the module bundle directly:// Swiftlet path = Bundle.module.path(forResource: \"StaticData\", ofType: \"json\")// Objective-CNSString *path = [SWIFTPM_MODULE_BUNDLE pathForResource:@\"StaticData\" ofType: @\"json\"];Passing the module bundle to APIs:// Swiftlet image = UIImage(named: \"Image\", in: .module)// Objective-CUIImage *image = [UIImage imageNamed:@\"Image\" inBundle: SWIFTPM_MODULE_BUNDLE];Best practices when accessing resources: Add each resource to the module that uses it If you need your resource to be visible from other modules, provide typed public accessors for individual resources Don’t vend the whole resource bundle as an API to avoid external dependenciesLocalized ResourcesAnother great addition to Swift Packages is the ability to add localized resources to your package.The very first thing to do is to set the default localization language in the package manifest:defaultLocalization: \"en\" Setting the default localization language will also be used as a fallback in case no better match can be found at runtime.Next, we need to create a localization directory for our default localization: Create a localization folder i.e. en.lproj Create a Localizable.strings file Add you localized string keys Repeat the steps above for all other languages your package will support.Using localized strings in SwiftUI:Text(\"Settings\", bundle: .module)With SwiftUI we can easily customize the preview by adding an .environment override to see a preview for a specific locale:MySwiftUIView() .environment(\\.locale, .init(identifier: \"de\")) You don’t need to add the localization directories in the package manifest since the .lproj file suffix makes the purpose of the file clear.Binary FrameworksLast but not least, we now also have the ability to distribute Binary Frameworks as Swift Packages.This is especially useful for distributing closed-source binaries when you don’t want or cannot distribute your source code.Adding a binary dependencyAdding a binary dependency is no different than adding a regular source-based Swift Package dependency. You can either add your dependency using Xcode or by adding it manually to you package manifest:dependencies: [ .package(name: \"Firebase\", url: \"https://github.com/firebase/firebase-ios-sdk.git\", .branch(\"X.Y-spm-beta\"))]Distributing Binary Frameworks as a Swift PackageDistributing Binary Frameworks as Swift Packages requires that you specifying the following in the targets section of your package manifest:targets: [ .binaryTarget( name: \"BinaryExample\", url: \"https://example.com/binary-example-1.0.0.xcframework.zip\", checksum: \"a40d8b15ed737c5029bf3d99fa0c46a6\")]A couple of notes regarding binary targets Binary targets use XCFrameworks under the hood Only supported on Apple platforms The URL of the binary can be HTTPS or path-based The binary will be downloaded separately when adding the dependency The name of the binary target corresponds to the module name Like source-based targets, use semantic versioning when versioning your packagesTo learn more about Binary Frameworks I highly suggest watching the Binary Frameworks in Swift video from WWDC19.Swift also includes a neat little tool to compute a checksum for your binary:swift package compute-checksum binary-example-1.0.0.xcframework.zip// outputs: a40d8b15ed737c5029bf3d99fa0c46a6SummaryWith all these new features introduced in Swift 5.3 and Xcode 12, there is not much holding Swift Packages back and I expect the adoption rate for Swift Packages to only increase over time.Looking ahead, I would like to see some built-in support for package discovery in Xcode. Fortunately, it seems like we might get some traction in this area soon with Proposal SE-0291.Additional Resources Swift packages: Resources and localization - WWDC20 Distribute binary frameworks as Swift packages - WWDC20 Binary Frameworks in Swift - WWDC19 Creating Great Localized Experiences with Xcode 11 - WWDC19" }, { "title": "Essential Xcode keyboard shortcuts for increased productivity", "url": "/posts/essential-xcode-keyboard-shortcuts-for-increased-productivity/", "categories": "xcode", "tags": "xcode, keyboard-shortcuts, productivity, tips, tricks", "date": "2020-07-08 00:00:00 +0200", "snippet": "As iOS developers, we spend a considerable amount of time in Xcode, so it’s worth taking some time to know your way around Xcode.Knowing how to edit, search effectively, and how to navigate your pr...", "content": "As iOS developers, we spend a considerable amount of time in Xcode, so it’s worth taking some time to know your way around Xcode.Knowing how to edit, search effectively, and how to navigate your project in Xcode goes a long way to increase your productivity.Taking the time to memorize and practice these shortcuts will save you a lot of time having to reach for your mouse and will allow you to keep both your hands on the keyboard, more often.Let’s have a look at a few essential keyboard shortcuts which is easy to remember yet will save you a lot of time once you’ve memorized them.Editor shortcutsWe spend the majority of our time in Xcode using the editor, writing code. So it only makes sense to know a couple of basic keyboard shortcuts when writing and editing your code.Moving lines up and down (⌥ + ⌘ + [ and ])For the longest time I was selecting the lines of code, cutting it, go to the line where I want to move it to and paste it there. Somewhere in between also required some backspacing and entering to make space for the lines pasted.With these two shortcuts, you can do it all in one go, a great time saver especially rearranging and moving code around in SwiftUI.Balance indentation (⌃ + I)Another useful keyboard shortcut to help you keep your code clean and neatly formatted. This is especially handy given how quickly indentation in SwiftUI can get out of control.Navigation shortcutsKnowing how to navigate effectively within Xcode can help you switch contexts quickly. It can also help you find what you looking for without manually searching for it.Quickly open (⇧ + ⌘ + O)This keyboard shortcut is the one I use most frequently, all the time. Instead of manually navigating and searching for a file in the project navigation you can use the open quickly keyboard shortcut.It works great for just about anything you want to navigate to, including files, types, functions, etc.Jump to definition (⌃ + ⌘ + J)Another keyboard shortcut I use often is to jump to the definition of a type, function, etc. Simply set your cursor on, for example, the type or function that you what to jump to and press ⌃ + ⌘ + J.Views shortcutsAdditional views like the project navigator, inspectors, preview, and debug area can be useful but not always needed. Often times they take up a lot of screen real estate when not needed, especially when you’re working on a smaller screen. Knowing how to hide and show these views as needed can free up a lot of screen space for the editor, allowing you to see more of your code.Toggle canvas / preview (⌥ + ⌘ + ↩)This keyboard shortcut is extremely useful when working with SwiftUI when you want to show/hide the preview.Open library (⇧ + ⌘ + L)Another frequent action is to open the library to insert code snippets, images, add view modifiers, etc.Using the library is also a great way to see what modifiers are available for a given view in SwiftUI.ConclusionMastering these keyboard shortcuts might seem a bit overwhelming at first. But once you’ve got then down it almost becomes natural and you’ll find yourself using them without thinking about it.Great for keeping both your hands on the keyboard more often making you a lean, mean code typing machine!Below is a table and overview of some of the most useful and frequently used keyboard shortcuts in Xcode: ⌘ = Command ⌥ = Option/Alt ⇧ = Shift-⌃ = Control ←→ ↑↓ = Arrow keys ↩ = Enter Category Command Shortcut Editor Go to beginning of line ⌘ + ← Editor Go to end of line ⌘ + → Editor Move line up ⌥ + ⌘ + [ Editor Move line down ⌥ + ⌘ + ] Editor Code completion ⌃ + Space Editor Comment line / selection ⌘ + / Editor Balance indentation ⌃ + I Editor Delete line ⌘ + D Editor Open library ⇧ + ⌘ + L Editor Generate documentation ⌥ + ⌘ + / Navigation Open quickly ⇧ + ⌘ + O Navigation Switch tabs left ⇧ + ⌘ + [ Navigation Switch tabs right ⇧ + ⌘ + ] Navigation Show current file in project navigator ⇧ + ⌘ + J Navigation Go forward in navigation history ⌃ + ⌘ + → Navigation Go back in navigation history ⌃ + ⌘ + ← Navigation Jump to definition ⌃ + ⌘ + J Navigation Go to line ⌘ + L Navigation Switch between code editors ⌘ + J Navigation Jump bar ⌃ + 1 - 6 Navigation Find selected symbol ⇧ + ⌃ + ⌘ + F Navigation Find call hierarchy ⇧ + ⌃ + ⌘ + H Views Toggle canvas / preview ⌥ + ⌘ + ↩ Views Toggle assistant ⌃ + ⌥ + ⌘ + ↩ Views Reload preview ⌥ + ⌘ + P Views Toggle project navigator ⌘ + 0 Views Toggle inspectors ⌘ + ⌥ + 0 Views Toggle debug area ⇧ + ⌘ + Y Debugging Toggle breakpoint ⌘ + \\ Debugging Step over F6 Debugging Step into F7 Debugging Step out F8 Debugging Continue to current line ⌃ + ⌘ + C Debugging Continue ⌃ + ⌘ + Y " }, { "title": "Resolving ITMS-90683 for standalone watchOS apps", "url": "/posts/resolving-itms-90683-for-standalone-watchos-apps/", "categories": "watchOS", "tags": "watchos, xcode, ITMS-90683", "date": "2020-06-10 00:00:00 +0200", "snippet": "Early on during the development of my recent standalone watchOS app while adding HealthKit support I stumbled into a very annoying issue. Upon uploading a TestFlight build the App Store Connect upl...", "content": "Early on during the development of my recent standalone watchOS app while adding HealthKit support I stumbled into a very annoying issue. Upon uploading a TestFlight build the App Store Connect upload API will fail with the following error:ITMS-90683: Missing Purpose String in Info.plistYour app’s code references one or more APIs that access sensitive user data.The app’s Info.plist file should contain an NSHealthShareUsageDescription keywith a user-facing purpose string explaining clearly and completely why your app needs the data.Naturally, I went back and made sure that I’ve added the NSHealthShareUsageDescription key in my Info.plist for my WatchKit Extension, and sure enough, it’s there. It seems for standalone watchOS apps you need to take an additional step when requesting HealthKit permissions. I’m yet to find this being documented anywhere in the Apple Developer Documentation and I’m hoping that the solution provided below will save you some time in the process.So, what’s the problem here?Whenever you are requesting permissions like reading the user’s HealthKit data you need to provide a reason for doing so. This is done by setting a string value for the NSHealthShareUsageDescription key in your Plist.info file.For iOS apps there is typically one Info.plist file in the root of your Xcode project, you set the NSHealthShareUsageDescription providing your reason/purpose for requesting permission and you’re off to the races. For watchOS apps it’s a little bit different, you have two Info.plist files, one for you WatchKit App and one for your WatchKit Extension. The main Info.plist is the one you find under your WatchKit Extension.Looking at the error description for ITMS-90683 it’s saying that hey, you haven’t set the NSHealthShareUsageDescription key in your Info.plist.Here’s how you resolve the issueThe biggest issue with the error description for ITMS-90683 is that it doesn’t tell you which Info.plist file it’s referring to making it very misleading and troublesome to debug.To resolve this, you simply need to manually add another Info.plist under the root level of your Xcode project as seen below:And then in your Info.plist you need to set the NSHealthShareUsageDescription key for the reason/purpose for requesting the permission:That’s it, that will satisfy the App Store Connect upload API and you should be able to upload your App Store / TestFlight builds.ConclusionIt seems just like iOS apps you need to have an Info.plist file containing the HealthKit usages keys like NSHealthShareUsageDescription to be present at the project root level.In the future, I hope Apple will update the Xcode template for standalone watchOS apps to include the additional Info.plist file in the project root or at the very least document this small, yet very important detail somewhere in the documentation." }, { "title": "Swift Development on the iPad using Terraform and Digital Ocean", "url": "/posts/swift-development-on-the-ipad-using-terraform-and-digital-ocean/", "categories": "swift, ipad", "tags": "swift, ipad, remote-development, terraform, digital-ocean, ssh, mosh", "date": "2020-02-11 00:00:00 +0200", "snippet": "Since the release of iPadOS 13, the iPad has gained a lot of useful features making it so much more of a viable option for getting real work done. That being said, it still lacks in several areas, ...", "content": "Since the release of iPadOS 13, the iPad has gained a lot of useful features making it so much more of a viable option for getting real work done. That being said, it still lacks in several areas, especially in the area of Swift development.Sure, Apple does have its own Swift Playground app, but as the name suggests, it is nothing more than a playground to learn and for prototyping concepts. Although Xcode on the iPad will be a dream come true for many of us, I don’t see that happening anytime soon.In this post, we’ll take a look at how to set up a remote machine for Swift development on the iPad with Terraform and Digital Ocean.Prerequisites A Digital Ocean account The Blink shell emulator app (or similar) A Mac And of course, an iPadSetting up a Digital Ocean accountDigital Ocean, like AWS, offers a wide range of cloud products and services, ranging from VPS aka Droplets to Kubernetes Clusters. Head on over to their website and create your account .Create your Personal Access TokenOnce you have your Digital Ocean account created, you need to create your Personal Access Token which you will later need when creating your infrastructure using Terraform.You can create a new Personal Access Token on Digital Ocean by navigating to Manage &gt; API and then clicking on the button to the right “Generate New Token”. Note that you need to enable both the READ and WRITE scope when creating your token.Make sure you copy and save your token somewhere safe for easy access later.Generating and adding your SSH KeysIn order to access our machine via SSH we need to generate a new set of SSH key pairs for both our Mac and iPad.On your MacOn macOS this can easily be done by running the following command:ssh-keygen -oThis will generate a new public and private key pair and our public key is what we will need to add on Digital Ocean. Copy the contents of ~/.ssh/id_rsa.pub or copy it to the clipboard with: pbcopy &lt; ~/.ssh/id_rsa.pub.Next, we need to add our public key on Digital Ocean under the Account &gt; Security &gt; SSH keys section. Click on the “Add SSH Key” button and paste in your public key.Now would also be a good time to take note of the Fingerprint generated for your SSH key which you’ll need later.On your iPadOn the iPad, there are a number of different apps that you can use to generate a new SSH key pair including Prompt, Termius, Working Copy and Blink shell (my personal favorite).Using Blink shell you can either generate a new SSH key pair by simply typing ssh-keygen from the prompt or by going to Settings &gt; Keys and adding it there. Tap on the keys you just added and select “Copy Public Key”.Repeat the steps for adding the public key for your Mac on Digital Ocean and remember to also take note of the generated fingerprint.Creating your VPS using TerraformIn case you are not yet familiar with Terraform, Terraform uses Infrastructure as Code to provision and manage cloud infrastructure and services. It automates rather tedious tasks when creating and setting up cloud infrastructure.Of course, we can also set up everything manually by hand but using Terraform to automate the entire process offers a lot of benefits as you will see in a minute.Installing TerraformTerraform offers a binary package for just about every platform, for more information see the installation guide.On macOS I prefer installing most of my tools using Homebrew which is as simple as running the following command:brew install terraformInfrastructure codeLet’s create a new terraform script called dev-machine.tf:# Variable declarationvariable \"do_token\" {}variable \"pub_key\" {}variable \"pvt_key\" {}variable \"ssh_fingerprint\" {}# Configure the DigitalOcean Providerprovider \"digitalocean\" { token = \"${var.do_token}\"}# Provider setupresource \"digitalocean_droplet\" \"web\" { image = \"ubuntu-18-04-x64\" name = \"ubuntu-dev\" region = \"fra1\" size = \"s-1vcpu-1gb\" private_networking = true ssh_keys = [ var.ssh_fingerprint, ] # Connection setup connection { user = \"root\" type = \"ssh\" host = self.ipv4_address private_key = file(var.pvt_key) timeout = \"2m\" } # Provisioning provisioner \"file\" { source = \"provision.sh\" destination = \"/tmp/provision.sh\" } provisioner \"remote-exec\" { inline = [ \"chmod +x /tmp/provision.sh\", \"/tmp/provision.sh\", ] }}Let’s break down each section of the terraform script above:Variable declarationWe declare variables at the beginning of the file that we will reference later throughout the script. Here we create variables for our Digital Ocean Personal Access Token, public key, private key, and SSH fingerprints.Provider setupThe provider section tells Terraform that we’re going to use Digital Ocean as our provider. Terraform also supports other providers like AWS, Azure, Linode, and many more. Here we create a very small VPS (droplet) with 1 CPU, 1GB of RAM, 25GB of SSD storage as defined by size = \"s-1vcpu-1gb\". Remember to also set your region accordingly, in the example it is set to Frankfort, Germany region = \"fra1\".Connection setupIn the connection section, we define how Terraform will access our newly created machine when running the provision script over SSH.ProvisioningOnce we have our newly created machine up and running we can start automating the process of installing the necessary tools and packages by running the provisioning script.Create a new bash script file called provision.sh in the same directory as dev-machine.tf:#!/bin/bash# updateapt -y update &amp;&amp; apt -y install unattended-upgrades# dependenciesapt -y install libcurl3 libpython2.7 libpython2.7-dev# toolsapt -y install curlapt -y install moshapt -y install clang# swiftwget https://swift.org/builds/swift-5.0.1-release/ubuntu1804/swift-5.0.1-RELEASE/swift-5.0.1-RELEASE-ubuntu18.04.tar.gztar xzf swift-5.0.1-RELEASE-ubuntu18.04.tar.gzmv swift-5.0.1-RELEASE-ubuntu18.04 /usr/share/swiftecho \"export PATH=/usr/share/swift/usr/bin:$PATH\" &gt;&gt; ~/.zshrcrm swift-5.0.1-RELEASE-ubuntu18.04.tar.gz# vaporeval \"$(curl -sL https://apt.vapor.sh)\"apt -y install vapor# optional toolsapt -y install zshchsh -s $(which zsh)sh -c \"$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\" \"\" --unattendedapt -y install tmux# firewall rulessudo ufw allow 60000:61000/udpLet’s take a minute to examine what exactly the script does:Updating system packagesFirst things first, we do a simple apt update and apt upgrade to get our system up to date.Installing dependenciesNext, we install all the dependencies needed for the tools and packages that we’re going to install.Installing toolsWe need to make sure that we also install all the tools that we’re going to need.Swift &amp; VaporNow that we have all the required packages and tools installed we can finally proceed with installing both Swift and Vapor.Optional toolsThis part of the script is entirely up to you and of course optional. I do however suggest that you install them and familiarise yourself with them, especially Tmux which will make your development experience on the iPad so much more enjoyable.Since we won’t be able to run an IDE, the next best option would be to customize and improve the experience while working in a shell environment. To achieve this I also recommend installing the ZSH shell together with OHMyZSH.Firewall rulesFinally, we need to open the inbound and outbound UDP ports on port 60000 and 61000 which is needed by Mosh. Mosh is a remote terminal application that allows roaming, supports intermittent connectivity and provides intelligent local echo and line editing of user keystrokes. It is a great alternative to SSH while working on the iPad when a stable connection is not always guaranteed.Creating the infrastructureOn your Mac, run the following two commands in terminal:export DO_TOKEN=YOUR-PERSONAL-ACCESS-TOKEN-HERE replacing your Personal Access Token\\export SSH_FINGERPRINT=YOUR-MAC-SSH-FINGERPRINT replacing your SSH fingerprint of your MacNext we need to initialise our Terraform script:terraform initBefore you go ahead and run the Terraform script, it is always a good idea to first run terraform plan. This will print out a diff of what Terraform is about to do without actually doing it, allowing you to make sure everything is correctly configured.Finally, and where all the magic happens, we can apply our changes by running:terraform apply \\-var \"do_token=$DO_TOKEN\" \\-var \"pub_key=$HOME/.ssh/id_rsa.pub\" \\-var \"pvt_key=$HOME/.ssh/id_rsa\" \\-var \"ssh_fingerprint_mac=YOUR-MAC-SSH-FINGERPRINT\" \\-var \"ssh_fingerprint_ipad=YOUR-IPAD-SSH-FINGERPRINT\"Wrapping upThat’s it, once completed, Terraform will create the infrastructure, install all the tools, and correctly configure everything for us to connect to our remote development machine.On Digital Ocean you should now see your newly created Droplet with a publicly accessible IP address and hostname.You can now go ahead and connect from your iPad using either SSH or Mosh. Simply add a new host using either the public IP address or hostname and connect via SSH or Mosh. Using Blink shell you can connect with Mosh running mosh your-server-name." }, { "title": "Swift Package Manager - Creating Packages", "url": "/posts/swift-package-manager-creating-packages/", "categories": "swift", "tags": "swift, spm, swift-package-manager, packages", "date": "2020-01-08 00:00:00 +0200", "snippet": "Swift Package Manager, or SPM, is a tool for managing the distribution of Swift code. It is also a great tool to manage your project dependencies, allowing you to import third-party frameworks or t...", "content": "Swift Package Manager, or SPM, is a tool for managing the distribution of Swift code. It is also a great tool to manage your project dependencies, allowing you to import third-party frameworks or those you developed yourself.Now that SPM is a first-class citizen as from Xcode 11 and we are starting to see some adoption, I think it is time to have a deeper look from both a consumer and a library author perspective.In this post, we’ll have a look at how to get started using SPM creating a basic package that we can share with others or use as a dependency in your own projects.What is a package?A Swift Package is essentially a collection of source files and assets compiled and package up into a single Module, which can, in turn, be added as a dependency in another project. Your project may contain one or multiple packages.Creating a Swift packageLet’s assume that we want to create an encryption/decryption library that we’d like to share with others. The first step when creating a new package is to create a new folder with the name of the package that you’d like to create, for example:mkdir CipherKitcd CipherKitThe next step is to generate the actual package:swift package init NOTE: Remember to name the folder accordingly as SPM will use the name of the folder as the actual package name when you don’t specify the name of the package manually. To manually specify the name of the package, use the --name parameter**Types of packagesBy default, SPM will initialize a static library package type when running the default initialize command.You can manually specify the type of package you’d like to create using the --type parameter. As of the time of writing this post the following types of packages are supported: empty library executable system-module manifestFor example, we can initialize a new executable package using the following command:swift package init --type executableYou can run the following command to get a list of available options when initializing a new package:swift package init --helpOVERVIEW: Initialize a new packageOPTIONS: --name Provide custom package name --type empty|library|executable|system-module|manifestThe manifest fileRunning the initialize command will create the initial structure of our package, including the manifest file contained in Package.swift. The manifest files contain important information about the package’s metadata, targets, products, and external dependencies.Have a look at the contents of Package.swift which should look something like this:vi Package.swift// swift-tools-version:5.1// The swift-tools-version declares the minimum version of Swift required to build this package.import PackageDescriptionlet package = Package( name: \"CipherKit\", products: [ // Products define the executables and libraries produced by a package, and make them visible to other packages. .library( name: \"CipherKit\", targets: [\"CipherKit\"]), ], dependencies: [ // Dependencies declare other packages that this package depends on. // .package(url: /* package url */, from: \"1.0.0\"), ], targets: [ // Targets are the basic building blocks of a package. A target can define a module or a test suite. // Targets can depend on other targets in this package, and on products in packages which this package depends on. .target( name: \"CipherKit\", dependencies: []), .testTarget( name: \"CipherKitTests\", dependencies: [\"CipherKit\"]), ])Adding external dependenciesNext, let’s add a remote dependency to our CipherKit library using another open-source crypto library called CryptoSwift, hosted on GitHub:import PackageDescriptionlet package = Package( name: \"CipherKit\", products: [ // Products define the executables and libraries produced by a package, and make them visible to other packages. .library( name: \"CipherKit\", targets: [\"CipherKit\"]), ], dependencies: [ // Dependencies declare other packages that this package depends on. // .package(url: /* package url */, from: \"1.0.0\"), .package (url: \"https://github.com/krzyzanowskim/CryptoSwift.git\", from: \"1.3.0\") ], targets: [ // Targets are the basic building blocks of a package. A target can define a module or a test suite. // Targets can depend on other targets in this package, and on products in packages which this package depends on. .target( name: \"CipherKit\", dependencies: []), .testTarget( name: \"CipherKitTests\", dependencies: [\"CipherKit\"]), ])Alternatively, you can also use a local path to refer to a package rather than a published remote version. Simply replace url with path using the relative path to the package’s folder: .package (url: \"../MyPackageFolder\")Finally you can also refer to a specific branch:.package (url: \"https://github.com/krzyzanowskim/CryptoSwift.git\", .branch(\"master\"))or an exact commit:.package (url: \"https://github.com/krzyzanowskim/CryptoSwift.git\", .revision(\"a44caef0550c346e0ab9172f7c9a3852c1833599\"))A note on dependency versioningIn the example above where we add a remote dependency to another framework, we specify a version of 1.3.0. Using Semantic Versioning, SPM will automatically resolve the most recent version between 1.3.0 and 2.0.0 (the next major version number).You may however want to lock into a specific version, perhaps due to a regression introduced in a later version. In cases like this, you can specify the .exact version of the package:.package (url: \"https://github.com/krzyzanowskim/CryptoSwift.git\", .exact \"1.3.0\")Building your packageNow that we have our dependencies defined, we need to build our package.To build your package run:swift buildThe initial build might take a few minutes to complete depending on the number of external dependencies you have. SPM will pull all the dependencies and then build and link those to your library. Your package is now ready for distribution and consumption.Working in XcodeUp until this point, we have only been working in terminal to create and build our package. For very basic libraries we could perhaps manage using just an editor like vim or vscode but we’d still lack the more advanced functionally that Xcode offers, for example, code completion and debugging support.Fortunately, we can also build and debug our library using Xcode. To do so we need to generate an Xcode project:swift package generate-xcodeprojThis will generate a new .xcodeproj file that we open with Xcode just like and other project with full code completion and debugging support. Remember to regenerate your Xcode project every time you add or edit your dependencies.**It is also important to note that your Package.swift remains the source of truth not the Xcode project file.AdvantagesOne of the main advantages of using SPM over CocoaPods or Carthage for example is the built-in support Xcode provides. Yes, that’s right, as from Xcode 11 SPM is now a first-class citizen with full support for creating and managing dependencies.Another advantage is that you don’t have to install any additional tools or deal with compatibility issues between different versions of Xcode. SPM is supported out of the box meaning that we have one less tool to install and keep up-to-date.DisadvantagesSPM still being relatively new and while only recently saw major adoption still lacks a few key features. One of them being support for binary dependencies. SE-0272 is an open proposal which is currently in “Accepted with revisions” state and will hopefully be implemented in the next version of Swift.UPDATE: Distributing Binary Frameworks are now supported in Swift 5.3Resources Swift Packages - Official Apple documentation for Swift Packages swiftpm.co - A place to find Swift packages " }, { "title": "SwiftUI in one minute", "url": "/posts/swiftui-in-one-minute/", "categories": "swiftui", "tags": "swiftui", "date": "2019-06-14 00:00:00 +0200", "snippet": "SwiftUI, a declarative UI programming framework introduced by Apple during WWDC 2019 aims to provide a shorter path to building great user interfaces across Apple platforms.With SwiftUI you learn t...", "content": "SwiftUI, a declarative UI programming framework introduced by Apple during WWDC 2019 aims to provide a shorter path to building great user interfaces across Apple platforms.With SwiftUI you learn the concepts once and apply it everywhere across all platforms i.e. iOS, iPadOS, watchOS, macOS.To start playing around with SwiftUI you will need to have Xcode 11 installed alongside macOS 10.15.The BasicsViews are defined declaratively, in other words, you tell SwiftUI what your UI should look like without having to worry about the “how it’s done”. Before SwiftUI’s introduction we were using UIKit’s imperative paradigm, the complete opposite of declarative where you need to tell the framework both what and how it should be done.Views are typically composed of several small single-purpose views that are extremely light-weight. What is also really great about SwiftUI is that there is little to no performance overhead having all these single-purpose views as the framework takes care of performance optimization etc.Creating ViewsA view is nothing more than a struct conforming to the View protocol. The contents of your view go inside the body property:struct AlbumView: View { var body: some View { Text(\"U2 - Joshua Tree\") }}Let’s break this down to understand what is going on here. As mentioned before, AlbumView is a struct (view) conforming to the View protocol. Every view in SwiftUI must conform to the View protocol. The return type of body property is some View (a new feature in Swift 5.1 called opaque return types) meaning that body will return some type of View. This is helpful since SwiftUI doesn’t know or care about the exact type of view it will return. The body of a view must always return only one child view. Text(\"U2 - Joshua Tree\") is another view containing a text label.That in a nutshell is all there is to SwiftUI. There is obviously much more you can do with SwiftUI given all the primitive views and platform adaptivity to build rich user interfaces. SwiftUI also works great with the new Combine and Previews frameworks, giving you an unparalleled app building experience.Additional Resources 204 - Introducing SwiftUI: Building Your First App Apple Developer Documentation" }, { "title": "Functional Programming in Swift", "url": "/posts/functional-programming-in-swift/", "categories": "swift", "tags": "swift, functional-programming", "date": "2019-04-02 00:00:00 +0200", "snippet": "Functional programming is a programming paradigm and perhaps even more so a mindset which helps you structure your programs into separate functions. These functions take some input and return the c...", "content": "Functional programming is a programming paradigm and perhaps even more so a mindset which helps you structure your programs into separate functions. These functions take some input and return the computed output based on the given input. The most important aspects of functional programming are to prevent side effects and to avoid mutating global state.Once you start applying functional programming techniques you will find that it makes your code more predictable, safer, easier to debug, test, and maintain.This post covers functional programming at a high level, so it’s helpful to consider the concepts in real-world problems.Imperative vs FunctionalWith an imperative programming style, you write sequential code, one line after the other mutating and maintaining state along the way, something we should all be very familiar with.It looks something like this:var character = \"Superman\"var skillLevel = 500print(\"\\(character) with skill level: \\(skillLevel)\") // Superman with skill level: 500Although technically correct, a more functional way to re-write the above example would look like this:func superHero(character: String, skillLevel: Int) -&gt; String { return \"\\(character) with skill level: \\(skillLevel)\"}print(superHero(character: \"Superman\", skillLevel: 500)) // Superman with skill level: 500The two examples above produce the same output but with the functional approach, you express your intentions with a function that makes it easier to read and to understand what it is supposed to do. The function only acts on the input and in turn produces and returns an output.Side effects and immutabilityIt is important to remember that with functional programming you are trying to avoid side effects. A side effect is anything a function might do which isn’t computing the output from the input given and return that output. In the case of the example below we are mutating the global state from within the function:var dcCharacters = [[\"Superman\", 500], [\"Batman\", 400], [\"Joker\", 300]]print(dcCharacters) // [[\"Superman\", 500], [\"Batman\", 400], [\"Joker\", 300]]func increaseSkillLevelOfCharacter(index: Int, increaseBy: Int) { if let currentSkillLevel = dcCharacters[0][1] as? Int { dcCharacters[index][1] = currentSkillLevel + increaseBy }}increaseSkillLevelOfCharacter(index: 0, increaseBy: 100)print(dcCharacters) // [[\"Superman\", 600], [\"Batman\", 400], [\"Joker\", 300]]Mutating and maintaining the global state using this approach will cause some undesired side effects as you constantly need to keep track of when and where you have changed the global state. In multi-threaded applications, this can lead to race conditions, deadlocks any other strange behavior which is often hard to debug.These side effects can be prevented by adhering to a few simple rules: Your functions should only rely on its own input Your functions should not mutate or change elements outside of themselves Your functions should always return some outputPure functionsIt all boils down to keeping your functions “pure”. A pure function is the opposite of side effects. It only depends on the input given, compute on that, and returns the output, nothing else. A function is considered to be pure if the output returned is always the same given the same input, making it predictable.As shown in a previous example, the following function is a pure function:func superHero(character: String, skillLevel: Int) -&gt; String { return \"\\(character) with skill level: \\(skillLevel)\"}print(superHero(character: \"Superman\", skillLevel: 500)) // This will always produce the same result given the same inputHere the superHero function is only concerned about its own input which in turn computes and return the output based on the input given. The output of this function will always be the same given the same input, again making it predictable.Dangers of in-place mutationAs we’ve seen, in-place mutation is never a good idea and leads to all sorts of unpredictable behavior which is hard to debug. More often than not you make assumptions of what the data should look like but somewhere in your code you mutated a value and forgotten about it.Ever found yourself trying to debug an issue which simply doesn’t make any sense and often hard to replicate? Chances are that some in-place mutation for example on a background queue causes a race condition, a common issue which might not always be that obvious.A common workaround is to make mutable copies of your data structures instead. Going back to one of the earlier examples we could create a mutable copy of our array and mutate that instead:let dcCharacters = [[\"Superman\", 500], [\"Batman\", 400], [\"Joker\", 300]]var dcCharactersCopy = dcCharactersdcCharactersCopy[0][1] = 1000Persistent data structures for efficient immutabilityOur dcCharacters array in the above example is now a constant making it immutable and we now have copy we can work with. But not only is this inefficient in terms of memory allocation it also introduces a new set of challenges having to maintain the state of the new mutable copy throughout the rest of the application. This can quickly escalate into a state management nightmare and can get messy rather quickly.Enter persistent data structures. A persistent data structure is a data structure allowing you to always preserves the previous version of itself when it is modified. Using the correct persistent data structure, you no longer have to create an entirely new copy of the data structure just to mutate for example one value in an array.The usage of persistent data structures is beyond the scope of this article, but I would suggest diving deeper into hash trees and linked lists.First-class and higher-order functionsA higher-order function is a function that has as its input and/or output, other functions. Functions in Swift are first-class values, i.e. functions may be passed as arguments to other functions, and functions may return new functions, also known as closures. This makes Swift a great language choice for functional programming.Avoid iteration and loopsIn functional programming, we’d like to avoid manual iteration and looping over items whenever possible. The most common way to iterate and loop over a collection would look something like this:var onlyDcSuperheros: [SuperHero] = [SuperHero]()for superhero in superheros { if superhero.world == .dc { onlyDcSuperheros.append(superhero) }}Swift provides several built-in higher-order functions but the most common are filter, map, and reduce which does all the heavy lifting for you.filterAs the name suggests, filter will iterate over a collection and filter out only those items passed as a parameter in the closure, for example, filter out only dc superhero characters in the superheros array:let superman = SuperHero(name: \"Superman\", skillLevel: 500, world: .dc)let batman = SuperHero(name: \"Batman\", skillLevel: 400, world: .dc)let joker = SuperHero(name: \"Joker\", skillLevel: 300, world: .dc)let ironman = SuperHero(name: \"Iron Man\", skillLevel: 450, world: .marvel)let captainamerica = SuperHero(name: \"Captain America\", skillLevel: 550, world: .marvel)let thor = SuperHero(name: \"Thor\", skillLevel: 350, world: .marvel)let superheros = [superman, batman, joker, ironman, captainamerica, thor]let dcSuperheros = superheros.filter { $0.world == .dc }mapApplies transformations to each item in the collection. Here we increase the skillLevel by 100 for each of the superhero characters:let improvedSuperHeros = superheros.map { $0.skillLevel + 100 }reduceReduces a collection into a single result, in the example below calculating combinedSkillLevel for all the superhero characters in our array:let combinedSkillLevel = dcSuperheros.reduce(0) { (result, superhero) in result + superhero.skillLevel}SummaryAs with any other programming paradigm, functional programming is not a “one size fits all” solution, and depending on the problem you are trying to solve it might not always the best choice. If however, you need to solve complicated problems at scale where the safety, predictability, and ease of maintenance is key then you might want to consider functional programming.Additional ResourcesSwift and the Legacy of Functional Programming by Rob NapierPersist data structure" }, { "title": "Swift and Objective-C documentation with Jazzy", "url": "/posts/swift-objective-c-documentation-with-jazzy/", "categories": "swift, objective-c, documentation", "tags": "swift, objective-c, jazzy, documentation, xcode", "date": "2019-03-23 00:00:00 +0200", "snippet": "Jazzy is a command-line utility that generates documentation for both Swift and Objective-C. Instead of parsing your source files, jazzy hooks into Clang and SourceKit to use the AST representation...", "content": "Jazzy is a command-line utility that generates documentation for both Swift and Objective-C. Instead of parsing your source files, jazzy hooks into Clang and SourceKit to use the AST representation of your code and its comments for more accurate results. With a little bit of manual effort and some markdown goodness, you can generate great-looking HTML documentation using jazzy.InstallationTo install jazzy simply install the ruby gem:sudo gem install jazzyAdding code documentationIn order to have the documentation generated by jazzy you first need to add code documentation using Xcode’s documentation markup and/or standard markdown. Before getting started you might also want to have a look at the following resources: Xocde Markup Formatting Reference Markdown Cheatsheet by Adam PritchardThe documentation you add to your source files will be used by jazzy in order to generate the HTML documentation. In the example below I have added code documentation for a class, struct, enum, and two functions, one of them being marked as deprecated:/** A simple HTTP user client - Author: Rynaard Burger */import Foundation/// User HTTP clientpublic class UserApiClient { /// Representation of a user public struct User: Codable { let firstname: String let lastname: String let age: Int /// User status /// /// - active: User is active /// - blocked: User is blocked enum Status { case active, blocked } } /** Creates a new user - Parameter user: The new user ### Usage Example: ### ```` UserApiClient.newUser(user1) ```` */ public static func newUser(_ user: User) { fatalError(\"not yet implemented\") } /// Creates a new user (Deprecated) /// /// - Parameters: /// - firstname: User firstname /// - lastname: User lastname /// - age: User age /// - warning: Deprecated. Use: `UserApiClient.newUser(_ user: User)` @available(*, deprecated, message: \"Use newUser(_ user: User)\") public static func newUser(firstname: String, lastname: String, age: Int) { fatalError(\"not yet implemented\") }} NOTE: You can also use the ⌥ (option) + ⌘ (command) + / keyboard shortcut while on the line of a class, struct, function, etc. to automatically add the code documentation for you.Swift and Objective-C documentation is written in markdown but also supports a number of special keywords, some of which includes:Swift: - warning - see - noteObjective-C: @warning @see @noteGenerating the HTML documentationGenerating the documentation is really easy, just go to the folder where your Xcode project is located and run the following command:jazzyThis will generate Swift documentation using the default options but you can actually do a whole lot more. You can always have a look at all the options available by running: jazzy -h.For example, you can specify the author, link to the author’s website, link to a GitHub page etc:Swiftjazzy \\ --clean \\--author \"Rynaard Burger\" \\--author_url https://rynaard.com \\--github_url https://github.com/rynaardb/jazzy-documentationObjective-CFor Objective-C you must also pass the following parameters to jazzy: --objc --umbrella-header ... --framework-root ... --sdk [iphone|watch|appletv][os|simulator]|macosx (optional, default value of macosx) --hide-declarations [objc|swift] (optional, hides the selected language declarations)What you get is beautiful looking HTML documentation very similar to Apple’s official documentation. All with just a few lines of code documentation and markdown:Other benefitsAnother benefit of adding code documentation is that you also get full autocomplete and Quick Help support right in Xcode with no additional effort required.To view the Quick Help you can use ⌥ (option) + right-click on a type or function, or alternatively you can also click on the “Show Quick Help inspector” button on the right in Xcode.SummaryThere is nothing worse than having to search through header files or actual implementation files just to figure out how things fit together. Having good documentation, especially for APIs or frameworks consumed by others, makes it so much easier to get started and to quickly figure out how everything is supposed to work together.Using jazzy there is no excuse why you cannot have nice looking documentation with very little effort.The example Xcode project can be found on my GitHub repository." }, { "title": "iOS animations using Lottie", "url": "/posts/ios-animations-using-lottie/", "categories": "ios", "tags": "ios, swift, lottie, animations", "date": "2019-03-16 00:00:00 +0200", "snippet": "Lottie is a mobile library for iOS and Android that parses Adobe After Effects animations exported as JSON with bodymovin and renders the vector animations natively on mobile and through React Nati...", "content": "Lottie is a mobile library for iOS and Android that parses Adobe After Effects animations exported as JSON with bodymovin and renders the vector animations natively on mobile and through React Native!Animations, especially complex animations can take a long time to implement by hand when using Core Animation or even just simple UIView animations. Using Lottie can save you a lot of time and manual effort. Using Lottie is easy, so let’s get started!Setting up LottieThe easiest way to install and use Lottie is by using CocoaPods, but there are other options too. We start by creating a Podfile in the same directory as our Xcode project and add the lottie-ios pod:target 'LottieAnimations' do use_frameworks! pod 'lottie-ios'endWith the lottie-ios pod added we simply need to do install the pod:pod installCocoaPods will generate a new Xcode workspace adding both your existing project and a new project called Pods. Remember to open the Xcode workspace and not the project.Working with LOTAnimationViewAnimations in Lottie is exported to JSON from Adobe After Effect and can be loaded from your app’s bundle or a URL. In the example below we are loading a file from the bundle called “loading.json”:let loadingAnimation = LOTAnimationView(name: \"loading\")With Lottie animations, you have full control over when to start, stop, or whether you’d like to play the animation in a loop. In it’s simplest form you can play the animation once:@objc private func animateOnce() { loadingAnimation.play { (finished) in print(\"Animation completed!\") } }In some cases, you might want to do a little bit more than just playing the animation once and this is where Lottie shines, giving you full control over the animation. You can easily loop, start, stop, or just play a portion of the animation:@objc private func loopAnimation() { loadingAnimation.loopAnimation = true loadingAnimation.play(fromProgress: 0, toProgress: 0.76, withCompletion: nil)}@objc private func stopAnimation() { loadingAnimation.stop()}The example Xcode project can be found on my GitHub repository.Lottie FilesFor inspiration check out all the animations shared by very talented motion designers on LottieFiles the animation used in my example was also sourced from LottieFiles." }, { "title": "Microservices with Vapor", "url": "/posts/microservices-with-vapor/", "categories": "swift, vapor", "tags": "vapor, server-side, swift, microservices", "date": "2019-02-23 00:00:00 +0200", "snippet": "Microservices have recently got a lot of attention and Vapor provides us with all the needed tools and APIs to create a Microservices architecture relatively easy.Vapor has several packages providi...", "content": "Microservices have recently got a lot of attention and Vapor provides us with all the needed tools and APIs to create a Microservices architecture relatively easy.Vapor has several packages providing support for the following: HTTP Redis SQL, MySQL, and PostgreSQL WebSocket Auth JWTWhat is Microservices Architecture?A definition of Microservices from Wikipedia:Microservices are a software development technique—a variant of the service-oriented architecture (SOA) architectural style that structures an application as a collection of loosely coupled services.But why?The topic around monolith vs microservice architecture has already been debated in the software design and architecture community and is beyond the scope of this post.Microservices offers several benefits some of which includes: Clear separation of concerns Each service can be deployed independently Promotes continuous integration and delivery Easier to test (when implemented correctly) Works great with Docker and Kubernetes Scalable Enables Agile developmentAPI GatewayAs illustrated below, the API Gateway is the external interface and entry point to communicate with all the internal services, similar to the Facade pattern in object-orientated design:It is the API Gateway’s responsibility to check and route each request to the corresponding internal services. The API Gateway might have other responsibilities such as authentication, access control, monitoring, logging, caching, load balancing, request shaping, etc.Implementing a Basic API Gateway in VaporThe Gateway Controller will check each request and route it to the corresponding service:func boot(router: Router) throws { router.post(\"users\", use: handle)}A quick and simple solution is to check the URL of the request for a prefix, in this case checking whether the request contains the /users prefix:func handle(_ req: Request) throws -&gt; Future&lt;Response&gt; { if req.http.urlString.hasPrefix(\"/users\") { guard let usersHost = Environment.get(\"USERS_HOST\") else { throw Abort(.badRequest) } return try handle(req, host: usersHost) } throw Abort(.badRequest)}The request is then handled by creating a new HTTP client pointing to correct host and endpoint:func handle(_ req: Request, host: String) throws -&gt; Future&lt;Response&gt; { let client = try req.make(Client.self) guard let url = URL(string: host + req.http.urlString) else { throw Abort(.internalServerError) } req.http.url = url req.http.headers.replaceOrAdd(name: \"host\", value: host) return client.send(req)}Implementing a Microservice in VaporEach service in the Microservices architecture would expose their REST API that’s consumed by other services or by the application’s clients. Some services might also implement a Web UI. At runtime, each service is often containerized using Docker and have its database, configurations, etc.The following diagram illustrates how each of the microservices communicates with each other:Each functional area of the application is implemented by its microservice exposing a REST API consumed by other services. For example, the user service invokes the order service to retrieve order information for the given user.Services might also use asynchronous, message-based communication for inter-service communication using for example the AMQP Protocol.So, let’s have a look at how a very basic user microservice can be implemented in Vapor.First, we need to configure the routes for the user service:public func routes(_ router: Router) throws { let userController = UserController() try router.register(collection: userController)}The user service has two endpoints, one to register a new user and one to retrieve an existing user:struct UserController: RouteCollection { func boot(router: Router) throws { let usersRouter = router.grouped(\"users\") usersRouter.post(CreateUserRequest.self, at: \"\", use: register) usersRouter.post(GetUserRequest.self, at: \"login\", use: getUser) }}Below is an example of how you can register a new user using PostgreSQL:private extension UserController { func register(_ req: Request, createRequest: CreateUserRequest) throws -&gt; Future&lt;HTTPStatus&gt; { let userRepository = try req.make(UserRepository.self) return try userRepository.findCount(username: createRequest.username, email: createRequest.email, on: req).flatMap { count in guard count == 0 else { throw Abort(.badRequest, reason: \"A user with these credentials already exists.\") } try createRequest.validate() let bcrypt = try req.make(BCryptDigest.self) let hashedPassword = try bcrypt.hash(createRequest.password) let user = User(username: createRequest.username, email: createRequest.email, password: hashedPassword) return try userRepository.save(user: user, on: req).transform(to: HTTPStatus.created) } }}As with every other architecture out there, there is no silver bullet and microservices are no exception. The Microservices architectures have many drawbacks ranging from added deployment complexity to testing which should be taken into consideration during the planning phases of your project. With that said it is still a great choice for complex, evolving applications despite drawbacks and implementation challenges.Here are links to my GitHub repositories containing the example code: vapor-microservices-api-gateway vapor-microservices-user-service" }, { "title": "Basic UIView Animations", "url": "/posts/basic-uiview-animations/", "categories": "ios", "tags": "ios, swift", "date": "2019-02-16 00:00:00 +0200", "snippet": "Subtle and concise animations can help draw the user’s attention to a specific area of your application. Often it may be unclear to the user when to perform a certain action or as to what is going ...", "content": "Subtle and concise animations can help draw the user’s attention to a specific area of your application. Often it may be unclear to the user when to perform a certain action or as to what is going on after a specific task / action was completed. This is where well thought out animations can be very useful.UIView.animateThere are several different APIs you can use to create animations but the most common and also very easy to use API is UIView.animate.Animating properties on a view can be done by calling UIView.animate where you need to specify the TimeInterval (duration) the animation will take to complete. Below simply animates the alpha channel of the view from 1 (visible) to 0 (completely transparent):func animateToothless() { UIView.animate(withDuration: 1) { self.leftTooth.alpha = 0 self.rightTooth.alpha = 0 }}The example above demonstrates a simple once-off animation but more often than not you would need to set some properties or do something else once the animation completes. For this there is another overload providing a completion callback once the animation completes:func animateWink() { let originalHeight = rightEye.frame.size.height let originalYPos = rightEye.center.y UIView.animate(withDuration: 0.3, animations: { self.rightEye.frame.size.height = 0 self.rightEye.center.y = 400 }) { (true) in self.rightEye.frame.size.height = originalHeight self.rightEye.center.y = originalYPos }}Above captures the original height and y position of the view before the animation starts and later on sets the view’s properties back to the original values once the animation completes.Additionally, one can also animate views using a timing curve corresponding to the motion of a physical spring. With this overload you can get rather creative by making very unique and realistic animations:func animateToungh() { let originaHeight = toungh.frame.size.height UIView.animate(withDuration: 1, delay: 0, usingSpringWithDamping: 0.2, initialSpringVelocity: 5, options: .curveEaseOut, animations: { self.toungh.frame.size.height = 55 }) { _ in self.toungh.frame.size.height = originaHeight }}When used correctly animations can delight your users and add overall polish to your app.A copy of the Xcode Playground can be found on my GitHub repository." }, { "title": "Getting started with Vapor", "url": "/posts/getting-started-with-vapor/", "categories": "swift, vapor", "tags": "vapor, server-side, swift", "date": "2019-02-08 00:00:00 +0200", "snippet": "Getting started with Vapor is easy thanks to the handy Vapor Toolbox and well written documentation.Prerequisites Swift 4.1 (bundled with Xcode) Xcode 9.3 or higher (optional if you install Swift...", "content": "Getting started with Vapor is easy thanks to the handy Vapor Toolbox and well written documentation.Prerequisites Swift 4.1 (bundled with Xcode) Xcode 9.3 or higher (optional if you install Swift manually)Installing VaporThe preferred method of installation is via brew.To install Vapor run to following commands in Terminal:brew tap vapor/tapbrew install vapor/tap/vaporCreating a new Vapor ProjectWith Vapor now installed, you can now create a new project:vapor new api-gatewayThis will use the default --api template but you can also use the --web template for example:vapor new web-ui --webBuilding your projectNext you will need to build your project:vapor buildThe initial build will take some time as it needs to download and compile all the dependencies. Subsequent builds will be much quicker.Running your projectNow that your project is built, you can simply run it with the following command:vapor runWith your server running you can expect to see the following output:Running default command: .build/debug/Run serveServer starting on http://localhost:8080Working with XcodeSince Vapor leans on Swift Package Manager you don’t need Xcode to get started with Vapor. You could use any Text Editor of your choice especially if you’re not running macOS. If you are however running macOS you would most probably want to use Xcode by creating an Xcode project:vapor xcodeThis will generate an Xcode project for you allowing you to run and debug your application just like you would do for any other Xcode project.That’s it! You are now ready to write Server-Side applications using Swift." } ]
